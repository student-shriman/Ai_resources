{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Deep Learning ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To understand what Deep Learning is, we first need to understand the relationship that Deep learning has with Machine learning,\n",
    "     Neural Networks, and Artificial Intelligence.\n",
    " \n",
    "> The best way to think of this relationship is to visualize them as concentric circles.\n",
    "\n",
    "<img src='images/ai.png' width='300px'>\n",
    "\n",
    "\n",
    "> At the outer most ring you have Artificial Intelligence.\n",
    "    Second layer inside of Artificial Intelligence is Machine learning and Deep learning is inside the Machine learning at the centre.\n",
    "    \n",
    "    Broadly speaking, Deep Learning is a more approachable name for an Artificial Neural Network. \n",
    "    The “Deep” in Deep Learning refers to the depth of the network. An Artificial Neural Network can be very shallow.\n",
    "\n",
    "> Neural networks are inspired by the structure of the cerebral cortex.\n",
    "     At the basic level is the perceptron, the mathematical representation of a biological neuron. \n",
    "         Like in the cerebral cortex, there can be several layers of interconnected perceptrons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Diagram of an Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  There are 3 different types of layers lies inside a Neural Networks.\n",
    "    \n",
    "        * Input Layer    *  Hidden Layers    *  Output Layer\n",
    "    \n",
    " =>  In Input Layer, Number of Perceptrons depends upon the total number of Features available in dataset.\n",
    "    \n",
    " =>  Perceptrons of Input Layer receives the inputted data. In above case, Row 1 is going to be inputted in \n",
    "      Input Layer, then 2nd Row, 3rd Row and so on .\n",
    "    \n",
    " =>  The main function of Input layer is just only getting the inputs from dataset.\n",
    "\n",
    " =>  This Inputted data goes to 2nd Layer of Neural networks (Hidden layer) in such a way that Data from each \n",
    "      Perceptron of Input layer goes inputted to each and every Perceptrons of Hidden layer.\n",
    "\n",
    " =>  When we try to pass our data from Input layer to Hidden layer, We multiply some randomly selected weights,\n",
    "        W1 to inputs X1 and then add some randomly selected Bias, B1 to the inputs. \n",
    "      Then this Linear mix of inputs (W1.X1 + B1) get entered into each Perceptrons of Hidden layer.\n",
    "        \n",
    " =>  Weights and Bias are learnable parameters and they are added to get patterens by making relations \n",
    "       in dataset more easily.\n",
    "\n",
    " =>  If we have only 2 Layers i.e. Input Layer and Output Layer, Whatever data we passes to Input Layer will\n",
    "      be outputted based on Output Functions and with no parameters it is very difficult to find the \n",
    "       relationships between the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/nn.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feed - Forward Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #############   #############   FFN in Training of Neural networks   ################   ##############\n",
    "    \n",
    " =>  Training of an Artificial Neural Networks has been classified into two diferent processes.\n",
    "       \n",
    "         * Feed Forward Connection (FFN)       &          * Backward Propagation\n",
    "        \n",
    " =>  In Feed Forward Connections, Data is getting inputted into the Input Layer o\n",
    "       Neural Network and then \n",
    "       it get multiplied and added with some randomly selected parameters like as 'weights' and 'Bias'.\n",
    "     \n",
    " =>  Weights & Bias are some Learnable parameters and they are getting introduced into the Networks for\n",
    "       getting an ease of Acces in finding  Relations between data.\n",
    "    \n",
    " =>  After introducing Weights and Bias, a Linear mix of Inputs are getting inputted to the Perceptron of\n",
    "       next layer of Network and there will be similar inputs from all other perceptrons of Input Layer.\n",
    "     So a Summation of all Linear Mix of Inputs will be inputted to each perceptrons of Hidden Layer.\n",
    "    \n",
    " =>  Inside a Hidden layer, a Linear mix of data enters as Input, so we need some particular function which \n",
    "      will be able to Filter, Restrict, Normalize and Non-Linearize the Inputted dataset, which is passed \n",
    "       from Input layer to Hidden layer. ThereFore Activation Function comes into picture. \n",
    "        We can add as many Hidden layers but in a certain limit.\n",
    "        \n",
    " =>  Activation functions are kind of functions, which always helps Perceptrons to fire itself in certain\n",
    "      direction or in a certain range and also try to restricts some sort of dataset.\n",
    "    \n",
    "     Examples of Activation function - \n",
    "\n",
    "        *  Sigmoid Activation Function         *  Tanh Activation Function  \n",
    "        *  RelU Activation Function            *  Leaky RelU Activation function etc.\n",
    "        \n",
    " =>  We can use a Single Activation Function inside a Hidden Layer.\n",
    "      Activation function does some particular operations and give some outputs (say o1, o2), which get \n",
    "        multiplied with some different weights again and then enters into Next Layer.\n",
    "        \n",
    "        \n",
    " =>  In Output Layer, We again uses a Function which is known as Output Function. \n",
    "    Here all Inputs are get added together by using Output function and then we get a final output or\n",
    "      prediction. \n",
    " \n",
    " =>  Output Functions are similar to Activation functions but it always getting used in the output Layer\n",
    "      of the Network.\n",
    "    \n",
    " =>  There will be some differences between the Expected and Predicted outputs, and this difference is\n",
    "      termed as Loss or cost. \n",
    "        \n",
    " =>  This whole process is known as Feed Forward Connection or Feed Forward Networks.\n",
    "      In FFN, our Network does not try to learn something. Learning Never happens in FFN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Assigning Weights and Bias To Inputs   \n",
    "\n",
    "<img src='images/weight.png' width='720px'>\n",
    "   \n",
    "    \n",
    "##   Input of Perceptrons in Hidden layer   \n",
    "\n",
    "<img src='images/total-input.jpg' width='480px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  In FFN, we get some Losses due to introducing some randomly selected parameters and our whole \n",
    "       objective and idea is to reduce this Loss. For reducing this loss, Backward Propagation comes \n",
    "        into picture.\n",
    "    \n",
    " =>  Backward Propagation is a method for efficiently computing the gradient of the cost function of a \n",
    "      Neural network with respect to its parameters. These partial derivatives can then be used to \n",
    "       update the parameters using 'Gradient Descent'.\n",
    "\n",
    " =>  The gradient shows how much the parameters 'weights' & 'Biases' needs to change\n",
    "       (in positive or negative direction) to minimize the Cost function 'C' .\n",
    "    \n",
    " =>  In Backward Propagation, it will try to update the randomly selected parameters by using some \n",
    "      'optimization' mechanism and it always occurs in backward direction and updates the parameters \n",
    "     layer-by-layer in Network on the basis of losses.\n",
    "      For updating the parameters, it uses a Differential equation.\n",
    "        \n",
    " =>  With updated parameters, it will start the Feed Forward Connection again and calculates some Losses\n",
    "      and then it again starts the Backward Propagation to reduce the weights. \n",
    "    It will keep updating the weights untill we get an optimal or reduced Loss and we get some pattern in\n",
    "      dataset.\n",
    "        \n",
    " =>  One Feed Forward connection and one Backward Propagation togetherly makes a cycle, known as one Epoch.\n",
    "\n",
    " =>  In Differential Eqns to update the parameters, we uses a Learning Rate to control the learning \n",
    "      process and Learning Rate should not be too high or too low, it will be in range of (0.001 to 10).\n",
    "        \n",
    " =>  At beginning, We dont have any idea about any relation or patterns between the dataset, so we \n",
    "       introduces some Learnable Parameters and in FFN, a Linear mix of Inputs goes to each and every\n",
    "      units of the Hidden Layer. In Backward Propagation, We updates the  parameters on the basis of Loss.\n",
    "        \n",
    " =>  So with updated parameters, We repeat the FFN and this time Our network drops some data, which are \n",
    "      not contributing in the result and this way, it learns the pattern between the dataset.\n",
    "    \n",
    " =>  Increment of Learnable Parameters makes our Network more flexible to learn the relationship\n",
    "      b/w dataset. Introducing more units in Hidden Layers & more Hidden Layers will give a better result\n",
    "     but they are just experimental ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Chain Rule for updating the parameters\n",
    "\n",
    "The chain rule is a way to compute the derivative of a function whose variables are themselves functions of other variables.  If $C$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another scalar variable $w$, then the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial z}\\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "For scalar-valued functions of more than one variable, the chain rule essentially becomes additive.  In other words, if $C$ is a scalar-valued function of $N$ variables $z_1, \\ldots, z_N$, each of which is a function of some variable $w$, the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\sum_{i = 1}^N \\frac{\\partial C}{\\partial z_i}\\frac{\\partial z_i}{\\partial w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/eqn.jpg' width = '420px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "In the following derivation, we'll use the following notation:\n",
    "\n",
    "$L$ - Number of layers in the network.\n",
    "\n",
    "$N^n$ - Dimensionality of layer $n \\in \\{0, \\ldots, L\\}$.  $N^0$ is the dimensionality of the input; $N^L$ is the dimensionality of the output.\n",
    "\n",
    "$W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ - Weight matrix for layer $m \\in \\{1, \\ldots, L\\}$.  $W^m_{ij}$ is the weight between the $i^{th}$ unit in layer $m$ and the $j^{th}$ unit in layer $m - 1$.\n",
    "\n",
    "$b^m \\in \\mathbb{R}^{N^m}$ - Bias vector for layer $m$.\n",
    "\n",
    "$\\sigma^m$ - Nonlinear activation function of the units in layer $m$, applied elementwise.\n",
    "\n",
    "$z^m \\in \\mathbb{R}^{N^m}$ - Linear mix of the inputs to layer $m$, computed by $z^m = W^m a^{m - 1} + b^m$.\n",
    "\n",
    "$a^m \\in \\mathbb{R}^{N^m}$ - Activation of units in layer $m$, computed by $a^m = \\sigma^m(h^m) = \\sigma^m(W^m a^{m - 1} + b^m)$.  $a^L$ is the output of the network.  We define the special case $a^0$ as the input of the network.\n",
    "\n",
    "$y \\in \\mathbb{R}^{N^L}$ - Target output of the network.\n",
    "\n",
    "$C$ - Cost/error function of the network, which is a function of $a^L$ (the network output) and $y$ (treated as a constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/Ann.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural Networks Training Blog](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Activation Functions are a kind of function, which helps a Neuron to fire itself in a certain direction, in a  certain bandwidth, or in a certain range. It also try to restrict some sort of dataset.\n",
    "    \n",
    "> Activation functions helps to determine the output of a Neural Network. These type of functions are attached to each Neurons in the Network, and determines whether it should be activated or not, based on whether each neuron’s input is relevant for the model’s prediction. \n",
    "\n",
    "> Activation Function also helps to normalize the output of each neuron to a range between (1 and 0) or between \n",
    "    (1 and -1).\n",
    "\n",
    "> Neural Networks use non-linear Activation Functions, which can help the network learn complex data, compute and learn almost any function representing a question, and provide accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  There are 3 different types of Activation Functions available to be used in Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Activation functions**\n",
    "\n",
    "> **Linear Activation functions** always defines a Linear relationship  'Y = m.X + c'. Differentials of a Linear function gives some constant, so it can be used in Back-propagation. But a linear activation function is not able to handle the complexity of data like as it will get failed in cases of Image recognitions or more. \n",
    "     \n",
    "<img src='images/linear.png' width='240px'>\n",
    "     \n",
    "**Binary Step Activation functions**\n",
    "\n",
    "> **Binary Step Activation functions** always gives it's output in a range (0 and 1). For Negative X, it always gives 0 and for Positive X, i gives 1. It can handle 2 classes.\n",
    " But it's derivative is always 0, so this function is not proper for using in Back-propagation.\n",
    "       \n",
    " **Non-linear Activation functions**\n",
    "     \n",
    ">  Non Linear Activation functions are able to understand the complexity of dataset and also it gives some differentials, which gets used in Back-propagation to update the parameters. So We generally uses Non-linear Activation functions in Deep learning.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####  Some popular Activation functions used in Deep Learning\n",
    "\n",
    "     * Sigmoid Function                        * Tanh Function\n",
    "     * ReLU Function                           * Leaky RelU Function\n",
    "     * PReLU Function                          * ELU Function\n",
    "     * Softmax Function                        * SoftPlus Function\n",
    "     * Swish Function                          * Maxout Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Sigmod  or Logistic Activation Function\n",
    "\n",
    "> The Sigmoid function is the most frequently used Activation function in the beginning of Deep learning.\n",
    "> For every Input value, it gives Output in a range ∈ (0,1). \n",
    "\n",
    "<img src='images/sig.svg' width='180px'>\n",
    "\n",
    "\n",
    "####  Calculation of Derivative of Sigmoid function\n",
    "\n",
    "<img src='images/sigmoid-derivative.jpg' width='240px'>\n",
    "<img src='images/sigmoid-derivative.png' width='320px'>\n",
    "\n",
    "\n",
    "### Graph of Sigmoid Function and it's Derivative\n",
    "\n",
    "<img src='images/sig.jpg' width='360px'>\n",
    "\n",
    "> Advantages of Sigmoid Function : -\n",
    "\n",
    "1. Smooth gradient, preventing “jumps” in output values.\n",
    "2. Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "3. Clear predictions, i.e very close to 0 or 1.\n",
    "\n",
    "\n",
    "> Sigmoid has 4 major disadvantages:\n",
    "* Leads to Vanishing Gradient problem\n",
    "* Leads to Gradient Saturation or dispersing\n",
    "* Function output is not zero-centered\n",
    "* Time consuming  and Slower for computers due to using Exponential operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vanishing Gradient Problem\n",
    "\n",
    "> In Back Propagation, Derivatives of each previous layers have some contribution in updating the weights of next layers and We uses Chain rule to update the weights. And Derivatives of Sigmoid Functions ranges between (0 to 0.25).\n",
    "\n",
    "> When Sigmoid functions are getting used as Activation functions in Deep neural networks, it's Derivative are getting decreases during updating of weights in each layers. In case of a Deep neural networks, When Back-propagation reaches to the last layer, Derivative of Sigmoid Function gives a very small value (near to 0) and so there will be very minute change or no changes in the weights in the last layer of Network. This problem is known as Vanishing Gradient problem.  \n",
    "\n",
    "> In 1980s or 1990s, Sigmoid function is the only Activation function, that people uses and they don't have too many layers in their Neural networks. They just deal with 2 or 3 layered Neural networks to avoiding this Vanishing Gradient problem.\n",
    "\n",
    "> Visit [Blog](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)\n",
    "        [wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "        \n",
    "\n",
    "###  Non Zero - Centric Problem\n",
    " \n",
    "> Functions having it's centre lying on the origin are Termed as Zero - Centric Functions.\n",
    "    Sigmoid function is not  Zero - Centric Function.\n",
    "\n",
    ">  When we uses a Zero - Centric function as Activation Function, it  will try to control the changes or updates in gradients in backward direction. \n",
    "\n",
    ">  In case of a Non Zero-centric Function, Gradient updates will goes in different - different directions and it will make our optimisation much more harder because it is not able to maintain the consistency in Gradient updates and so every time it's value fluctuates and gives a different value.\n",
    "\n",
    "###   Gradient Saturation or Gradient Dispersing Problem\n",
    "\n",
    "> When inputs are away from the coordinate origin, the gradient of the function becomes very small, almost zero. \n",
    "In the process of neural network backpropagation, we all use the chain rule of differential to calculate the differential of each weight w. When the backpropagation passes through the sigmod function, the differential on this chain is very small. Moreover, it may pass through many sigmod functions, which will eventually cause the weight w to have little effect on the loss function, which is not conducive to the optimization of the weight. This The problem is called Gradient Saturation or Gradient Dispersion.\n",
    "\n",
    " [visit here](https://programmersought.com/article/21921360722/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Tanh or Hyperbolic Tangent Function\n",
    "\n",
    "> Tanh or Hyperbolic Tangent function is modified version of the Sigmoid Function. \n",
    "The output interval of tanh is [-1 & 1] and the whole function is 0-centric, which is better than Sigmoid.\n",
    "<img src='images/tan.svg' width='180px'>\n",
    "<img src='images/sig-tan.jpg' width='360px'>\n",
    "\n",
    "> Tanh function can be represented in terms of Sigmoid function.\n",
    "<img src='images/tan-sigmoid.jpg' width='340px'>\n",
    "\n",
    "####  Derivative of TanH Function\n",
    "\n",
    "> For calculating the derivative of Tanh Function, we uses u/v rule. \n",
    "<img src='images/tan.jpg' width='240px'>\n",
    "<img src='images/tan-derivative.jpg' width='340px'>\n",
    "\n",
    "> In general Binary classification problems, Tanh function is used for the hidden layer and the Sigmoid function is used for the output layer. However, these are not static, and the specific activation function to be used must be analyzed according to the specific problem, or it depends on debugging.\n",
    "\n",
    "> Comparing to Sigmoid Function, Tanh is a Zero centerd function but still it leads to Vanishing Gradient problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Rectified  Linear  Unit  or  ReLU  Function\n",
    "\n",
    "> ReLU or Rectified Linear Unit function is defined as a function that takes the maximum value and outputs in a range (0, +infinity), and this function is not fully interval-derivable, but we can take sub-gradients.  \n",
    "<img src='images/relu-fn.jpg' width='360px'>\n",
    "<img src='images/relu-der.jpg' width='360px'>\n",
    "<img src='images/relu.png' width='360px'>\n",
    "\n",
    "\n",
    "> Advantages of ReLU Function.\n",
    "*  Overcome with Gradient Saturation or Vanishing Gradient problem. When the input is Positive, it always gives a Gradient but in case of Negative input, it gives Y as 0, so no any Gradient.\n",
    "\n",
    "* ReLU Function not activates all Neurons at a time. Means that, If any Linear combination of inputs are negative, then it will restrict that Neuron to fire itself and gives output as 0. May be in Next iteration, if there will be Positive input, then it will Fire the Neuron. This property is very important in Feed Forward Connection. \n",
    "\n",
    "*  ReLU calculation speed is much faster because it has only a Linear relationship (forward or backward) and so it is much faster than Sigmod and Tanh. (Sigmod and tanh need to calculate the exponent, which results in slower calculation speed.)\n",
    "\n",
    "> Disadvantages of ReLU Function\n",
    " *  When the input is Negative, ReLU is completely inactive, which means that once a negative number is entered, ReLU will die. In this way, in the forward propagation process, it is not a problem. Some areas are sensitive and some are insensitive. But in the back propagation process, if you enter a negative number, the gradient will be completely zero, which has the same problem as the Sigmoid function and Tanh function.\n",
    "\n",
    " *  ReLU Function is not a Zero Centric function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Leaky  ReLU  Function\n",
    "\n",
    "> Leaky ReLU Function is a modified version of ReLU Function. For Negative Inputs, It gives some values of Y.\n",
    "In order to solve the Dead ReLU Problem, people proposed to set the first half of ReLU 0.01x instead of 0.\n",
    "\n",
    ">  Leaky ReLU has all the advantages of ReLU, plus there will be no problems with Dead ReLU, but in actual operation, it has not been fully proved that Leaky ReLU is always better than ReLU. \n",
    "\n",
    "\n",
    "<img src='images/leaky-relu.png' width='360px'>\n",
    "<img src='images/lrelu.svg' width='180px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PRelu (Parametric ReLU)\n",
    "\n",
    "> Parametric ReLU or PReLU Function is improvised version of ReLU Function. In the negative region, PReLU has a small slope, which can also avoid the problem of ReLU death. Compared to ELU, PReLU is a linear operation in the negative region. Although the slope is small, it does not tend to 0, which is a certain advantage.\n",
    "\n",
    "<img src='images/Capture.PNG' width='380px'>\n",
    "\n",
    "> In PRelu, alpha 'α' is a learnable parameter, which it learns by tuning..\n",
    "\n",
    "* if α = 0, f becomes ReLU\n",
    "* if α > 0, f becomes Leaky ReLU\n",
    "* if α is a learnable parameter, f becomes PReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ELU (Exponential Linear Units) function\n",
    "\n",
    "> ELU is also proposed to solve the problems of ReLU. \n",
    "\n",
    "<img src='images/elu.svg' width='280px'>\n",
    "<img src='images/elu.jpg' width='360px'>\n",
    "\n",
    "\n",
    ">  Advantages of ELU Function\n",
    "\n",
    "* No dead Neurons issues, if input is Negative\n",
    "* ELU is a Zero-centric Function means that Mean of the output is close to 0.\n",
    "\n",
    ">  Disadvantages of ELU Functions\n",
    "\n",
    "*  For negative inputs, ELU computation is expensively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "\n",
    "> Softmax function is used for Multi class classifications in output layers. It is used for getting the probabilities for different multiple classes among all. It always gives an output in a range [0,1].\n",
    "\n",
    "####  Softmax Function is defined as \n",
    "<img src='images/soft.png'>\n",
    "\n",
    "> Softmax is different from the normal max function: the max function only outputs the largest value, and Softmax ensures that smaller values have a smaller probability and will not be discarded directly. It is a \"max\" that is \"soft\".\n",
    "\n",
    "> The denominator of the Softmax function combines all factors of the original output value, which means that the different probabilities obtained by the Softmax function are related to each other.\n",
    "\n",
    "> In the case of binary classification, for Sigmoid, there are:\n",
    "<img src='images/soft1.png'>\n",
    "\n",
    "\n",
    "> For Softmax with K = 2, there are:\n",
    "<img src='images/soft2.png'>\n",
    "\n",
    "> It can be seen that in the case of binary classification, Softmax is degraded to Sigmoid.\n",
    "<img src='images/soft4.png' width='240px'>\n",
    "   \n",
    "###  Calculation of probabilities using Softmax Function  \n",
    "\n",
    "<img src='images/prob-softmax.jpg' width='480px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Swish (A Self-Gated) Function\n",
    "\n",
    ">  Swish Function is derived from the Sigmoid Function itself and it is defined as =>  **y = x * sigmoid (x)**\n",
    "\n",
    "> Swish's design was inspired by the use of sigmoid functions for gating in LSTMs and Highway networks. We use the same value for gating to simplify the gating mechanism, which is called **self-gating**. \n",
    "\n",
    "> The advantage of self-gating is that it only requires a simple scalar input, while normal gating requires multiple scalar inputs. This feature enables self-gated activation functions such as Swish to easily replace activation functions that take a single scalar as input (such as ReLU) without changing the hidden capacity or number of parameters.\n",
    "\n",
    "<img src='images/swish.png' width='360px'\n",
    "     >\n",
    "\n",
    "####  Advantages of Swish Function\n",
    "\n",
    "*  Unboundedness (unboundedness) is helpful to prevent gradient from gradually approaching '0' during slow training, causing Saturation. At the same time, being bounded has advantages, because bounded active functions can have strong regularization, and larger negative inputs will be resolved.\n",
    "\n",
    "* At the same time, smoothness also plays an important role in optimization and generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SoftPlus Activation Function\n",
    "\n",
    "> The softplus function is similar to the ReLU function, but it is relatively smooth.It is unilateral suppression like ReLU. It has a wide acceptance range (0, + inf).\n",
    "\n",
    "<img src='images/softPlus.jpg' width='240px'>\n",
    "<img src='images/softplus.png' width='360px'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Maxout Activation Function\n",
    "\n",
    "> The Maxout activation function don't goes with the linear combination inputs, but it gives the maximumm input among all different inputs coming from various Neurons.\n",
    "<img src='images/max.jpeg' width='240px'>\n",
    "\n",
    "> One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have w1,b1 =0).The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks\n",
    "\n",
    "\n",
    "> The Maxout activation is a generalization of the ReLU and the leaky ReLU functions. It is a learnable activation function.\n",
    "\n",
    "> Maxout can be seen as adding a layer of activation function to the deep learning network, which contains a parameter k. Compared with ReLU, sigmoid, etc., this layer is special in that it adds k neurons and then outputs the largest activation value. value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generally speaking, these activation functions have their own advantages and disadvantages. There is no statement that indicates which ones are not working, and which activation functions are good. All the good and bad must be obtained by experiments.\n",
    "\n",
    "> [Blog](https://medium.com/@omkar.nallagoni/activation-functions-with-derivative-and-python-code-sigmoid-vs-tanh-vs-relu-44d23915c1f4#:~:text=Like%20the%20sigmoid%20function%2C%20one%20of%20the%20interesting,of%20tanh%20%28z%29%3A%20a%3D%20%28e%5Ez-e%5E%20%28-z%29%29%2F%20%28e%5Ez%2Be%5E%20%28-z%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cost Functions used in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  L1 and L2 loss \n",
    "\n",
    "> *L1* and *L2* are two common Loss functions which are mainly used to minimize the error.\n",
    "\n",
    "   *   **L1 loss function** is also known as **Least Absolute Deviations** in short **LAD**.\n",
    "\n",
    "   *   **L2 loss function** is also known as **Least Square Errors** in short **LSE**.\n",
    "\n",
    "### L1 Loss function\n",
    "\n",
    "> It is used to minimize the error which is the sum of all the absolute differences in between the Actual value and the Predicted value.\n",
    "<img src=\"images\\img13.png\" width='480px'>\n",
    "\n",
    "\n",
    "### L2 Loss Function\n",
    "\n",
    "> It is also used to minimize the error which is the sum of all the squared differences in between the Actual value and the Predicted value.\n",
    "<img src=\"images\\img15.png\" width='480px'>\n",
    "\n",
    "> **L1 Loss is more sensitive to outliers and controls them, so it is always advisable to use L1 Loss in case of outliers, otherwise go with L2 Loss.** \n",
    "\n",
    "###  Python Implementation for L1 and L2 losses\n",
    "\n",
    ">  **l1_loss = tf.abs((y_pred - y_actual))**\n",
    "  \n",
    ">  **l2_loss = tf.square((y_pred - y_actual))**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Huber Loss \n",
    "\n",
    "> Huber Loss is a combination of L1 & L2 losses and this uses a threshold value to detect the outliers.\n",
    "  In case of outliers, it uses L1 Loss with some regularizations otherwise it uses the L2 loss. \n",
    "\n",
    "<img src=\"images\\huber-loss.jpg\" width='480px'>  \n",
    "\n",
    "\n",
    "> Huber Loss is often used in Regression problems. Compared with L2 loss, Huber Loss is less sensitive to outliers (because if the residual is too large, it is a Piecewise function, loss is a linear function of the residual).\n",
    "Among them, $\\delta$ is a set parameter, $y$ represents the real value, and $f(x)$ represents the predicted value.\n",
    "\n",
    "> The advantage of this is that when the residual is small, the loss function is L2 norm, and when the residual is large, it is a linear function of L1 norm.\n",
    "\n",
    "> The Huber loss is quadratic when the error is smaller than a thres‐hold δ (typically 1), but linear when the error is larger than δ. Thismakes it less sensitive to outliers than the mean squared error, andit is often more precise and converges faster than the mean abso‐lute error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Hinge Loss\n",
    " \n",
    "> Hinge loss is often used for Binary classification problems, such as ground true: t = 1 or -1, predicted value y = wx + b. \n",
    " \n",
    "> In other words, the closer the y is to t, the smaller the loss will be.\n",
    "\n",
    "<img src='images/hinge-loss.png'>\n",
    "\n",
    "> For getting more about [Hinge Loss ](https://en.wikipedia.org/wiki/Hinge_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cross Entropy Loss\n",
    "\n",
    "> Cross Entropy Loss is mainly applied to Binary classification problems. The predicted value is a probability value and the loss is defined according to the cross entropy. Note the value range of the above value: the predicted value of y should be a probability and the value range is [0,1] .\n",
    "\n",
    "<img src=\"images\\cross-entropy.jpg\" width='420px'>\n",
    "\n",
    "> Cross entropy loss is a commonly used loss function in machine learning and deep learning, especially in classification problems. It measures the difference between two probability distributions: the predicted distribution (often represented by a model's output) and the true distribution (often represented by the target labels).\n",
    "\n",
    ">In simple terms, the cross entropy loss calculates the difference between the predicted probability of the correct label and the actual probability of that label. The higher the difference, the higher the loss.\n",
    "\n",
    ">Mathematically, the cross entropy loss is defined as:\n",
    "H(p,q) = -∑(p_i * log(q_i))\n",
    "Where p is the true distribution, q is the predicted distribution, and i iterates over all possible classes. \n",
    "\n",
    ">The logarithmic term ensures that larger differences between p and q result in a higher loss. The cross entropy loss is commonly used with softmax activation functions to produce a probability distribution over the classes.\n",
    "\n",
    ">During the training process, the goal is to minimize the cross entropy loss by adjusting the model's parameters, which will improve the model's predictions over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Cross Entropy Loss vs Sigmoid Cross Entropy Loss vs Softmax Cross-entropy Loss  ..\n",
    "\n",
    "> Cross entropy loss, sigmoid cross entropy loss, and softmax cross entropy loss are all commonly used loss functions in machine learning and deep learning. While all of them are used in classification problems, they differ in the type of activation function used in the final layer of the neural network and the way the loss is calculated.\n",
    "\n",
    "> Cross entropy loss:\n",
    "Cross entropy loss measures the difference between the predicted and true probability distributions using the logarithmic term to penalize large differences. It is commonly used in multi-class classification problems.\n",
    "\n",
    ">Sigmoid cross entropy loss:\n",
    "Sigmoid cross entropy loss is used for binary classification problems. The sigmoid activation function is used in the final layer of the neural network to produce a probability score between 0 and 1 for a binary classification problem. The sigmoid cross entropy loss calculates the difference between the predicted and true probability distributions using the logarithmic term to penalize large differences.\n",
    "\n",
    ">Mathematically, sigmoid cross entropy loss is defined as:\n",
    "H(y, ŷ) = -(y * log(ŷ) + (1-y) * log(1-ŷ))\n",
    "Where y is the true label and ŷ is the predicted probability.\n",
    "\n",
    ">Softmax cross entropy loss:\n",
    "Softmax cross entropy loss is used in multi-class classification problems, where the output of the neural network is a probability distribution over multiple classes. The softmax activation function is used to convert the output of the neural network into a probability distribution over the classes. The softmax cross entropy loss calculates the difference between the predicted and true probability distributions using the logarithmic term to penalize large differences.\n",
    "\n",
    ">Mathematically, softmax cross entropy loss is defined as:\n",
    "H(y, ŷ) = -∑(y_i * log(ŷ_i))\n",
    "Where y is the true distribution and ŷ is the predicted distribution, and i iterates over all possible classes.\n",
    "\n",
    ">In summary, cross entropy loss is used for multi-class classification, sigmoid cross entropy loss is used for binary classification, and softmax cross entropy loss is used for multi-class classification with a softmax activation function in the final layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cross Entropy Loss vs categorical Cross entropy Loss vs Sparse Categorical cross entropy Loss  ..\n",
    "\n",
    "> Cross entropy loss, categorical cross entropy loss, and sparse categorical cross entropy loss are all used in classification problems. They differ in the format of the target labels and the way the loss is calculated.\n",
    "\n",
    "> Cross entropy loss:\n",
    "Cross entropy loss measures the difference between the predicted and true probability distributions using the logarithmic term to penalize large differences. It is commonly used in multi-class classification problems.\n",
    "\n",
    ">Categorical Cross entropy loss:\n",
    "Categorical cross entropy loss is used when the target labels are one-hot encoded vectors, where only one element of the vector is 1, and the rest are 0. In other words, each target label represents a unique class. This loss function is commonly used in multi-class classification problems.\n",
    "\n",
    ">Mathematically, categorical cross entropy loss is defined as:\n",
    "H(y, ŷ) = -∑(y_i * log(ŷ_i))\n",
    "Where y is the true one-hot encoded vector and ŷ is the predicted distribution, and i iterates over all possible classes.\n",
    "\n",
    ">Sparse categorical cross entropy loss:\n",
    "Sparse categorical cross entropy loss is used when the target labels are integers that represent the class of each sample. In other words, the target labels are not one-hot encoded vectors, but rather a single integer value. This loss function is also commonly used in multi-class classification problems.\n",
    "\n",
    ">Mathematically, sparse categorical cross entropy loss is defined as:\n",
    "H(y, ŷ) = -log(ŷ_k)\n",
    "Where y is the true integer label, k is the predicted class index, and ŷ is the predicted distribution over all possible classes.\n",
    "\n",
    ">In summary, cross entropy loss is used for multi-class classification, categorical cross entropy loss is used when the target labels are one-hot encoded vectors, and sparse categorical cross entropy loss is used when the target labels are integers that represent the class of each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Role of Optimisers in Deep Learning\n",
    "\n",
    "> Optimisers played a vital role in Neural Networks training. When data enters inside the network with Input Nodes, Feed Forward Connection starts and it will end up with getting some losses.\n",
    "\n",
    "> Then Backward Propagation starts and it updates the randomly selected parameters layer-by-layer in the Network, by using some 'optimization' mechanism in backward direction.\n",
    "   \n",
    "<img src=\"images\\diag.jpg\" width='480px'>\n",
    "\n",
    ">  There are 3 basic optimisers based on **Gradient Descent** approach, that we uses in Deep learning.\n",
    " \n",
    "     *  Batch Gradient Descent or (BGD)\n",
    "     *  Stochastic Gradient Descent or (SGD)\n",
    "     *  Mini Batch Gradient Descent or (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Batch Gradient Descent\n",
    "\n",
    "> BGD uses the entire Training set to calculate the **Gradient** of the **Cost function** to the parameters.\n",
    "\n",
    "> In BGD, We try to send the whole dataset at a time and Records are selected randomly and enters to the network one-by-one. Then we calculate a Combined Loss or Cost by taking Summation of all losses, and then we send the Cost for optimisation for finding out the Gradients. \n",
    "\n",
    "> In BGD, it will be able to do **Optimisation**, if it will be able to pass entire dataset to the **Neural network**.\n",
    "\n",
    "> **Batch Gradient Descent can converge to a global minimum for convex functions and to a local minimum for non-convex functions.**\n",
    "\n",
    ">  **Disadvantages of BGD**\n",
    "\n",
    "* Memory Consumption is too high\n",
    "* Calculation of Gradients will be slow\n",
    "* Computationally very expensive.\n",
    "* Not good to update the model in real time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Mini-batch Gradient Descent\n",
    "\n",
    "> In MBGD, We make batches of 'n' (n ε (50 ～ 256 advisable) records and then these mini-batches are selected randomly and enters to the Neural network one-by-one. Then we calculates Costs for each mini-batches and send for **optimisation**.\n",
    "\n",
    "> In a single Iteration, one mini-batch passes thru the Network and we calculate the Cost and send it for **optimisation** and then update the parameters. Then in next Iteration, some other mini-batches will be passes and we calculates the cost and update the weights.\n",
    " \n",
    "> MBGD uses a small batch of samples, that is, n samples to calculate each time. In this way, it can reduce the variance when the parameters are updated, and the convergence is more stable.\n",
    " It can make full use of the highly optimized matrix operations in the **Deep Learning** library for more efficient gradient calculations.\n",
    "\n",
    "> **MBGD is modified version of BGD but with multiple mini-batches with 'n ε (50 ～ 256)' records.\n",
    "\n",
    "> **In MBGD, if batch size is equal to Total no. of records in datset, it becomes BGD.**\n",
    "\n",
    "> **Compare to BGD, MBGD is resource efficient, consumes low memory and calculation is faster. \n",
    " \n",
    "**Disadvantages with Mini-Batch Gradient Descent**\n",
    "\n",
    "> **Mini-batch Gradient Descent** does not gives a guarantee that we will be able to do a good convergence of Data or Error in a better way, causes due to randomly selection of batches, because Samples are extracted from dataset don't represents the properties of Entire dataset and so we don't get a good Convergence and so also not a **Absolute Global Minima or Local Minima** points.\n",
    "\n",
    "> If **Learning rate, η** is tto small, my convergence rate falls and Time to take for finding out the Absolute minima will increase and in case of a high **Learning rate, η**, it will not be able to achieve Absolute Mnima and it will keep oscillating between Maximum values and minimum values, causes due to different errors getting for different mini-batches.  \n",
    "\n",
    "> We should control the **Learning rate, η** in case of MBGD.\n",
    "> In addition, this method is to apply the **same learning rate** to all parameter updates. If our data is sparse, we would prefer to update the features with lower frequency.\n",
    "\n",
    "> In addition, for **non-convex functions**, it is also necessary to avoid trapping at the local minimum or saddle point, because the error around the saddle point is the same, the gradients of all dimensions are close to 0, and SGD is easily trapped here.\n",
    "\n",
    "> **Saddle points** are the curves, surfaces, or hypersurfaces of a saddle point neighborhood of a smooth function are located on different sides of a tangent to this point.\n",
    "For example, this two-dimensional figure looks like a saddle: it curves up in the x-axis direction and down in the y-axis direction, and the saddle point is (0,0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stochastic Gradient Descent\n",
    "\n",
    "> In **Stochastic Gradient Descent**, a single record is selected and send to the **Neural networks**. We calculates the Loss for this single record and send it for **optimisation** and the update the weights.\n",
    " Then in next Iterations, some other records are sent to the network individually and we calculates the loss and the update the weights.\n",
    " \n",
    "> In SGD, **Loss function** and **optimisers** are not supposed to wait for the entire dataset to calculate themselves. \n",
    "\n",
    "> Comparing to BGD, SGD works just with a single iteration and so requires less computational resources but takes more time to train the network.\n",
    "\n",
    "> IN BGD, it will get converged to **Local Minima** point, but in SGD, it will always oscillates or vary between one point to other for each and every dataset, so it's very difficult to get **Absolute Minima** point.\n",
    "\n",
    "> IN SGD, gets multiple Minimum values for entire dataset and we get some **zig-zag** type curve and minima's will keep fluctuating.\n",
    "\n",
    "> For SGD, **Learning rate, η** should be lesser comparing to BGD and MBGd.\n",
    "\n",
    "> SGD is faster than BGD and MBGD.\n",
    "\n",
    "<img src='images/sgd.png' width='240px'>\n",
    " \n",
    " **<center>Fluctuations in SGD</center>** \n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "> However, because SGD is updated more frequently, the cost function will have severe oscillations.\n",
    "BGD can converge to a local minimum, of course, the oscillation of SGD may jump to a better local minimum.\n",
    "\n",
    "> When we decrease the learning rate slightly, the convergence of SGD and BGD is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BGD, MBGD and SGD are some ways to pass the data from Neural networks.\n",
    "\n",
    "<img src='images/gradient-update.jpg' width='240px'>\n",
    "\n",
    "###  Optimizers are ways to calculate the Gradients and update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept of Momentum in Deep Learning\n",
    "\n",
    "> **Momentum is momentum**, which simulates the inertia of an object when it is moving, that is, the direction of the previous update is retained to a certain extent during the update, while the current update gradient is used to fine-tune the final update direction. In this way, you can increase the stability to a certain extent, so that you can learn faster, and also have the ability to get rid of local optimization.\n",
    "\n",
    "> Using **Momentum-based Optimizers**, We get smooth convergence of data and takes very less time to calculate the gradients.\n",
    "\n",
    "<img src='images/sgd1.png' width='480px'>\n",
    " \n",
    " **<center>Figure :- SGD without Momentum &&&  SGD with Momentum</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Momentum-based Gradient Descent optimiser or AdaGrad\n",
    "\n",
    "> **Adagrad** is an algorithm for gradient-based optimization, which adapts the **Learning rates** to the parameters, using low **Learning rate** for parameters associated with frequently occurring features, and using high **Learning rates** for parameters associated with infrequent features. So, it is well-suited for dealing with **sparse data**.\n",
    "\n",
    "> In very beginning, We starts from a high **Learning rate** and gradually as I will increase my Iterations, I will keep decreasing my **Learning rate** because in very beginning, our **Neural network** will not be aware about dataset and Every time we receives new variances or records of data, but in Later stage, once my **Neural network** is able to pass data for certain epochs (or Iterations) and so my model has alreday adjusted the weights and if their will be some changes in features are not frequent, we will slow down the **Learning rate**.\n",
    "In this case, I am providing my model a better possibility to converge for optimise the Losses.\n",
    "\n",
    "> I will be able to define the **Accuracy** of my model, if it will converge and this way, we uses to keep changing the **Learning rate** and will not use a constant **Learning rate** throughout the entire learning period.\n",
    "\n",
    "> A constant **Learning rate** may not be suitable for all parameters. For example, some parameters may have reached the stage where only fine-tuning is needed, but some parameters need to be adjusted a lot due to the small number of corresponding samples.\n",
    "\n",
    "> Adagrad proposed as an algorithm that adaptively assigns different **Learning rates** to various parameters among them. \n",
    "\n",
    ">**GloVe word embedding uses AdaFrad where infrequent words required a greater update and frequent words require smaller updates.**\n",
    "\n",
    ">**AdaGrad eliminates the need to manually tune the learning rate.**\n",
    "\n",
    "<img src='images/adagrad.jpg' width='480px'>\n",
    "\n",
    "##  Disadvantages in Adagrad optimiser\n",
    "\n",
    "* **The learning rate is monotonically decreasing.**\n",
    "* **The learning rate in the late training period is very small.**\n",
    "* **It requires manually setting a global initial learning rate.**\n",
    "* **Computationally very Expensive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "\n",
    "> **Adadelta** is an extension of **Adagrad** and it tries to reduce Adagrad’s property of aggressively and monotonically reducing the **Learning rate**.\n",
    "\n",
    "<img src='images/adadelta.jpg' width='480px'>\n",
    "\n",
    "> In **AdaGrad optimiser**, we uses summation of squares of gradients from all previous epochs, but In **Adadelta optimiser**, we uses **root-mean-square** of gradient of this current epoch and uses **Learning rate** from previous epoch. \n",
    "\n",
    ">In **Adadelta**, we take the ratio of the **Running average** of the previous epochs to the current gradient and we do not need to set the default **Learning rate**.\n",
    "\n",
    "> So,this way! I am able to eliminate the dependencies of selecting a Hard-coded value as **Learning rate** and this **Learning rate** is dynamic and will not be decreased to a small value because learning rate will be changed in each & every epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RMSProp Optimiser\n",
    "\n",
    "> The full name of RMSProp algorithm is **Root Mean Square Propagation**, which is an adaptive learning rate optimization algorithm proposed by **Geoffrey Hinton**. \n",
    "\n",
    ">**Geoff Hinton** is known as **Father of Deep learning** and get **Alan Turing award** in 2018.\n",
    "\n",
    ">**RMSProp** tries to resolve Adagrad’s radically diminishing **Learning rates** by using a moving average of the squared gradient. It utilizes the magnitude of the recent gradient descents to normalize the gradient.\n",
    "\n",
    "<img src='images/rmsprop.jpg' width='480px'>\n",
    "\n",
    "> **Adagrad** will accumulate all previous gradient squares, but **RMSProp** just calculates the corresponding average value, so it can alleviate the problem that the learning rate of the **Adagrad algorithm** drops quickly.\n",
    "\n",
    "> The difference is that **RMSProp** calculates the **differential squared weighted average of the gradients** and this makes the network functions converge faster. \n",
    "\n",
    ">In **RMSProp**, **Learning rate** gets adjusted automatically and it chooses a different learning rate for each parameter.\n",
    "\n",
    ">**RMSProp** divides the learning rate by the average of the exponential decay of squared gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "**Adaptive Moment Estimation optimiser, Adam** is another method that computes adaptive learning rates for each parameter by storing an exponentially decaying average of past squared gradients like **Adadelta** and **RMSprop**.\n",
    "\n",
    "> **Adam** also keeps an exponentially decaying average of past gradients, similar to momentum.\n",
    "\n",
    "> **Adam** can be viewed as a combination of **Adagrad** and **RMSprop** so that **Adagrad** works well on sparse gradients and **RMSProp** works well in online and non-stationary settings repectively.\n",
    "\n",
    "> **Adam** implements the **Exponential moving average of the gradients** to scale the **Learning rate** instead of a simple average as in **Adagrad**. It keeps an exponentially decaying average of past gradients.\n",
    "\n",
    "> **Adam optimizer** is computationally efficient and has very less memory requirement and itis one of the most popular and famous **Gradient descent-based Optimization** algorithms.\n",
    "\n",
    "> **Adam optimizer** comes with **SGD**, by default but also can be used with other. \n",
    "\n",
    "<img src='images/adam.jpg' width='480 px'>\n",
    "<img src='images/adam1.jpg' width='480 px'>\n",
    "<img src='images/adam2.jpg' width=' 360px'>\n",
    "\n",
    "> **Uncentored Variances** are those variances which, are not able to be defined in both directions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
