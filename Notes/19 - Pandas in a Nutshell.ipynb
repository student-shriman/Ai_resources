{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        ###############    ##############    Pandas Tutorial    #############    ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         ##############    ############   Contents  ###############    #################\n",
    "    \n",
    "    *  Pandas Introduction & Advantages\n",
    "    *  Why Pandas is so popular in data science?\n",
    "    *  Pandas DataFrame & Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ###############    ##############   Pandas Introduction & Advantages    ###############    ############## \n",
    "\n",
    " =>  Pandas is an open-source library used for Data analysis and manipulation and it offers various operations \n",
    "       and data structures to perform numerical data manipulations and time series.\n",
    "    \n",
    " =>  It is created on top of 'Numpy' library.\n",
    "    \n",
    " =>  Pandas was initially developed by Wes McKinney in 2008 while he was working at AQR Capital Management.\n",
    "\n",
    " =>  Advantages of pandas -\n",
    "\n",
    "     * Fast and efficient for manipulating and analyzing data.\n",
    "\n",
    "     * Data from different file objects can be loaded.\n",
    "\n",
    "     * Easy handling of missing data (represented as NaN) in floating point as well as non-floating point data\n",
    "\n",
    "     * Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects\n",
    "\n",
    "     * Data set merging and joining.\n",
    "\n",
    "     * Flexible reshaping and pivoting of data sets\n",
    "\n",
    "     * Provides time-series functionality.\n",
    "\n",
    "     * Powerful group by functionality for performing split-apply-combine operations on data sets.\n",
    "        \n",
    " =>  Pandas comes inbuilt with Anaconda distribution or can be installed with the help of pip.\n",
    "        \n",
    "        $ pip install pandas\n",
    "        \n",
    " =>  Importing pandas with an alias ->    >>>  import pandas as pd\n",
    "\n",
    " =>  There are 2 types of Data structures in pandas.\n",
    "    \n",
    "           *  Dataframe        * series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Installing Numpy and Pandas with alias\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     #############    #########    Why pandas is so popular in data science?    ##########    ##############\n",
    "    \n",
    " =>  Pandas is used in conjunction with other libraries that are used for Data science.\n",
    "\n",
    " =>  It is built on the top of the NumPy library which means that a lot of structures of NumPy are used or \n",
    "       replicated in Pandas.\n",
    "        \n",
    " =>  The data produced by Pandas is often used as input for plotting functions of Matplotlib, \n",
    "        statistical analysis in SciPy, machine learning algorithm in Scikit-learn.\n",
    "    \n",
    " =>  Pandas is designed for working on Tabular Heterogeneous data, while Numpy is designed for working on\n",
    "         Homogeneous Numerical array data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "             #############   #############    Pandas Series    #############     ##############\n",
    "    \n",
    " =>  Pandas Series is a one-dimensional labeled array capable of holding data of \n",
    "         any type (integer, string, float, python objects, etc.).\n",
    "\n",
    " =>  The axis labels are collectively called index. \n",
    "    \n",
    " =>  Pandas Series is nothing but a column in an excel sheet. \n",
    "\n",
    " =>  Labels need not be unique but must be a hashable type. \n",
    "    \n",
    " =>  The object supports both integer and label-based indexing and provides a host of\n",
    "        methods for performing operations involving the index.\n",
    "    \n",
    " =>  A Pandas Series will be created by loading the datasets from existing storage, \n",
    "        storage can be SQL Database, CSV file, and Excel file. \n",
    "    \n",
    " =>  Pandas Series can be created from the lists, dictionary, and from a scalar value etc.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20200225170506/pandas-series.png\">   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "               ###########    ###########    Creating a Pandas Series   ##########    ############\n",
    "\n",
    " =>  A Pandas Series is created by loading the datasets from existing storage, storage can be SQL Database, \n",
    "        CSV file, and Excel file. \n",
    "    \n",
    " =>  An Empty Pandas Series is created with pd.series() method.\n",
    "\n",
    "        >>> series = pd.Series()\n",
    "    \n",
    " =>  Pandas Series can be created from the lists, dictionary, and from a scalar value etc. \n",
    "\n",
    "        >>> series = pd.Series(list/dictionary)\n",
    "      \n",
    " =>  In order to create a series from Numpy array, we have to import numpy module and have to use array() function.\n",
    "        \n",
    "        >>>  array = np.array(['g', 'e','e', 'k', 's'])\n",
    "        \n",
    "        >>>  series = pd.Series(array)\n",
    "        \n",
    " =>  To create a Pandas series with provided index , we uses index argument with a list.\n",
    "       \n",
    " =>  Data items and index list should have equal number of items.\n",
    "\n",
    "      >>>  series = pd.Series(array, index =[10, 11, 12, 13, 14])\n",
    "    \n",
    " => In order to create a series from scalar value, an index must be provided. The scalar value will be repeated \n",
    "          to match the length of index.\n",
    "    \n",
    "        >>>  series = pd.Series('value', index =[0, 1, 2, 3, 4, 5]) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a          1\n",
       "b          2\n",
       "c          3\n",
       "d          4\n",
       "e          5\n",
       "f          a\n",
       "g    Shriman\n",
       "h      India\n",
       "i     Python\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List = [1, 2, 3 , 4, 5, 'a', 'Shriman', 'India', 'Python']\n",
    "\n",
    "series = pd.Series(List, index=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'])\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ###############     ############    Pandas Dataframe    #############     #############\n",
    "    \n",
    " =>  Pandas DataFrame is two-dimensional size-mutable, potentially heterogeneous tabular data structure \n",
    "        with labeled axes (rows and columns).\n",
    "    \n",
    " =>  A Data frame is a two-dimensional data structure, i.e., data is aligned in a tabular fashion in rows \n",
    "       and columns. \n",
    "    \n",
    " =>  Pandas DataFrame consists of three principal components, Data, rows, and columns.\n",
    "\n",
    " =>  A Pandas DataFrame will be created by loading the datasets from existing storage,\n",
    "        storage can be SQL Database, CSV file, and Excel file and many more. \n",
    "        \n",
    " =>  Pandas DataFrame can be created from the lists, dictionary, and from a list of dictionary etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20200225170602/pandas-dataframe.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     ##############   ##########   Methods To creating DataFrames in Pandas    ##########   ##############   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       ###############   ##########   Dataframe from a Dictionary    ###########   ############## \n",
    "\n",
    " =>  Calling Dataframe constructor on lists or nested lists generates a DataFrame.\n",
    "\n",
    " =>  Single list gives a Single-column dataframe where as a Nested List gives a dataframe of multiple size.\n",
    "       Inner lists of a nested lists makes the rows and items of a list makes the columns of Dataframe.\n",
    "        \n",
    "      >>>  pd.DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]]\n",
    "    \n",
    " =>  Steps to create a Datframe with dictionaries or ndarray \n",
    "\n",
    "       *  To create DataFrame from dict of ndarray/list, all the ndarray must be of same length. \n",
    "    \n",
    "       * If index is passed then the length index should be equal to the length of arrays. \n",
    "            If no index is passed, then by default, index will be range(n) where n is the array length.\n",
    "            \n",
    "      >>>  dict = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n",
    "                    'year': [2000, 2001, 2002, 2001, 2002, 2003],\n",
    "                     'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\n",
    "\n",
    "      >>>  df = pd.Dataframe(dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "      <th>pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>2000</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>2001</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>2002</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>2001</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>2002</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>2003</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    state  year  pop\n",
       "0    Ohio  2000  1.5\n",
       "1    Ohio  2001  1.7\n",
       "2    Ohio  2002  3.6\n",
       "3  Nevada  2001  2.4\n",
       "4  Nevada  2002  2.9\n",
       "5  Nevada  2003  3.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dict = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'],\n",
    "                    'year': [2000, 2001, 2002, 2001, 2002, 2003],\n",
    "                     'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\n",
    "    \n",
    "df = pd.DataFrame(dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ###########   ########    Creation of dataframe using read_csv()   #########   ########## \n",
    "    \n",
    " *  pd.read_csv('filepath')  ->  This reads a CSV file and converts to a DataFrame.\n",
    "\n",
    " =>  We can use read_csv() to read a TSV file with delimiter = '\\t' .\n",
    "    \n",
    " *  df.to_csv(path, sep, ** kwargs) is used to store a dataframe to a csv file.  \n",
    " \n",
    "\n",
    "#  Reading a csv file manually in pandas\n",
    "import csv\n",
    "\n",
    "f = open('files/file.csv')\n",
    "csv = csv.reader(f)\n",
    "\n",
    "for i in csv:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('files/titanic.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ###########   ########    Creation of dataframe using read_table()   #########   ############ \n",
    " \n",
    " =>  pd.read_table('filepath')  ->  This creates a dataset from a Tab-separated values file.\n",
    "    \n",
    " =>  We can use read_table() to read a csv file with delimiter = ',' .\n",
    "\n",
    " =>  This uses all parameters of read_csv() method.\n",
    "    \n",
    "      >>>  pd.read_table('files/sms.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ham  \\\n",
       "0      ham   \n",
       "1     spam   \n",
       "2      ham   \n",
       "3      ham   \n",
       "4     spam   \n",
       "...    ...   \n",
       "5566  spam   \n",
       "5567   ham   \n",
       "5568   ham   \n",
       "5569   ham   \n",
       "5570   ham   \n",
       "\n",
       "     Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "0                         Ok lar... Joking wif u oni...                                                               \n",
       "1     Free entry in 2 a wkly comp to win FA Cup fina...                                                               \n",
       "2     U dun say so early hor... U c already then say...                                                               \n",
       "3     Nah I don't think he goes to usf, he lives aro...                                                               \n",
       "4     FreeMsg Hey there darling it's been 3 week's n...                                                               \n",
       "...                                                 ...                                                               \n",
       "5566  This is the 2nd time we have tried 2 contact u...                                                               \n",
       "5567               Will ü b going to esplanade fr home?                                                               \n",
       "5568  Pity, * was in mood for that. So...any other s...                                                               \n",
       "5569  The guy did some bitching but I acted like i'd...                                                               \n",
       "5570                         Rofl. Its true to its name                                                               \n",
       "\n",
       "[5571 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creatring a dataframe using read_tsv()\n",
    "\n",
    "df = pd.read_table('files/sms.tsv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ###########   ########    Creation of dataframe using read_clipboard()   #########   ########## \n",
    "    \n",
    " * pd.read_clipboard()  ->  This creates a dataFrame from the content in clipboard and pass to read_csv() method.\n",
    "\n",
    "     => sep  ->  (string values) Default is  '\\s+'  denotes one or more white-space characters.\n",
    "    \n",
    "     => **kwargs  ->  This parameter belongs to read_csv() method.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ###############    ############    Dataframe from a HTML Webpage    ############    ###############\n",
    "    \n",
    " =>  pd.read_html('URL')  ->  This is used to extract tables from Html webpage and returns a list.\n",
    "\n",
    " =>  This list contains all tables from a webpage with a separate index.\n",
    "    \n",
    " =>  To get Tables as Dataframe, We uses List Indexing.\n",
    "\n",
    "       >>> list = pd.read_html(\"https://basketball-reference.com/leagues/NBA_2015_totals.html\")\n",
    "       \n",
    "       >>> list[i] return the particular dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = pd.read_html(\"https://basketball-reference.com/leagues/NBA_2015_totals.html\")\n",
    "type(list)\n",
    "\n",
    "len(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rk</th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>FG</th>\n",
       "      <th>FGA</th>\n",
       "      <th>...</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quincy Acy</td>\n",
       "      <td>PF</td>\n",
       "      <td>24</td>\n",
       "      <td>NYK</td>\n",
       "      <td>68</td>\n",
       "      <td>22</td>\n",
       "      <td>1287</td>\n",
       "      <td>152</td>\n",
       "      <td>331</td>\n",
       "      <td>...</td>\n",
       "      <td>.784</td>\n",
       "      <td>79</td>\n",
       "      <td>222</td>\n",
       "      <td>301</td>\n",
       "      <td>68</td>\n",
       "      <td>27</td>\n",
       "      <td>22</td>\n",
       "      <td>60</td>\n",
       "      <td>147</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jordan Adams</td>\n",
       "      <td>SG</td>\n",
       "      <td>20</td>\n",
       "      <td>MEM</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>35</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>.609</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Steven Adams</td>\n",
       "      <td>C</td>\n",
       "      <td>21</td>\n",
       "      <td>OKC</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>1771</td>\n",
       "      <td>217</td>\n",
       "      <td>399</td>\n",
       "      <td>...</td>\n",
       "      <td>.502</td>\n",
       "      <td>199</td>\n",
       "      <td>324</td>\n",
       "      <td>523</td>\n",
       "      <td>66</td>\n",
       "      <td>38</td>\n",
       "      <td>86</td>\n",
       "      <td>99</td>\n",
       "      <td>222</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Jeff Adrien</td>\n",
       "      <td>PF</td>\n",
       "      <td>28</td>\n",
       "      <td>MIN</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>19</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>.579</td>\n",
       "      <td>23</td>\n",
       "      <td>54</td>\n",
       "      <td>77</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Arron Afflalo</td>\n",
       "      <td>SG</td>\n",
       "      <td>29</td>\n",
       "      <td>TOT</td>\n",
       "      <td>78</td>\n",
       "      <td>72</td>\n",
       "      <td>2502</td>\n",
       "      <td>375</td>\n",
       "      <td>884</td>\n",
       "      <td>...</td>\n",
       "      <td>.843</td>\n",
       "      <td>27</td>\n",
       "      <td>220</td>\n",
       "      <td>247</td>\n",
       "      <td>129</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>116</td>\n",
       "      <td>167</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>490</td>\n",
       "      <td>Thaddeus Young</td>\n",
       "      <td>PF</td>\n",
       "      <td>26</td>\n",
       "      <td>TOT</td>\n",
       "      <td>76</td>\n",
       "      <td>68</td>\n",
       "      <td>2434</td>\n",
       "      <td>451</td>\n",
       "      <td>968</td>\n",
       "      <td>...</td>\n",
       "      <td>.655</td>\n",
       "      <td>127</td>\n",
       "      <td>284</td>\n",
       "      <td>411</td>\n",
       "      <td>173</td>\n",
       "      <td>124</td>\n",
       "      <td>25</td>\n",
       "      <td>117</td>\n",
       "      <td>171</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>490</td>\n",
       "      <td>Thaddeus Young</td>\n",
       "      <td>PF</td>\n",
       "      <td>26</td>\n",
       "      <td>MIN</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>1605</td>\n",
       "      <td>289</td>\n",
       "      <td>641</td>\n",
       "      <td>...</td>\n",
       "      <td>.682</td>\n",
       "      <td>75</td>\n",
       "      <td>170</td>\n",
       "      <td>245</td>\n",
       "      <td>135</td>\n",
       "      <td>86</td>\n",
       "      <td>17</td>\n",
       "      <td>75</td>\n",
       "      <td>115</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>490</td>\n",
       "      <td>Thaddeus Young</td>\n",
       "      <td>PF</td>\n",
       "      <td>26</td>\n",
       "      <td>BRK</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>829</td>\n",
       "      <td>162</td>\n",
       "      <td>327</td>\n",
       "      <td>...</td>\n",
       "      <td>.606</td>\n",
       "      <td>52</td>\n",
       "      <td>114</td>\n",
       "      <td>166</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>56</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>491</td>\n",
       "      <td>Cody Zeller</td>\n",
       "      <td>C</td>\n",
       "      <td>22</td>\n",
       "      <td>CHO</td>\n",
       "      <td>62</td>\n",
       "      <td>45</td>\n",
       "      <td>1487</td>\n",
       "      <td>172</td>\n",
       "      <td>373</td>\n",
       "      <td>...</td>\n",
       "      <td>.774</td>\n",
       "      <td>97</td>\n",
       "      <td>265</td>\n",
       "      <td>362</td>\n",
       "      <td>100</td>\n",
       "      <td>34</td>\n",
       "      <td>49</td>\n",
       "      <td>62</td>\n",
       "      <td>156</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>492</td>\n",
       "      <td>Tyler Zeller</td>\n",
       "      <td>C</td>\n",
       "      <td>25</td>\n",
       "      <td>BOS</td>\n",
       "      <td>82</td>\n",
       "      <td>59</td>\n",
       "      <td>1731</td>\n",
       "      <td>340</td>\n",
       "      <td>619</td>\n",
       "      <td>...</td>\n",
       "      <td>.823</td>\n",
       "      <td>146</td>\n",
       "      <td>319</td>\n",
       "      <td>465</td>\n",
       "      <td>113</td>\n",
       "      <td>18</td>\n",
       "      <td>52</td>\n",
       "      <td>76</td>\n",
       "      <td>205</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>675 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rk          Player Pos Age   Tm   G  GS    MP   FG  FGA  ...   FT%  ORB  \\\n",
       "0      1      Quincy Acy  PF  24  NYK  68  22  1287  152  331  ...  .784   79   \n",
       "1      2    Jordan Adams  SG  20  MEM  30   0   248   35   86  ...  .609    9   \n",
       "2      3    Steven Adams   C  21  OKC  70  67  1771  217  399  ...  .502  199   \n",
       "3      4     Jeff Adrien  PF  28  MIN  17   0   215   19   44  ...  .579   23   \n",
       "4      5   Arron Afflalo  SG  29  TOT  78  72  2502  375  884  ...  .843   27   \n",
       "..   ...             ...  ..  ..  ...  ..  ..   ...  ...  ...  ...   ...  ...   \n",
       "670  490  Thaddeus Young  PF  26  TOT  76  68  2434  451  968  ...  .655  127   \n",
       "671  490  Thaddeus Young  PF  26  MIN  48  48  1605  289  641  ...  .682   75   \n",
       "672  490  Thaddeus Young  PF  26  BRK  28  20   829  162  327  ...  .606   52   \n",
       "673  491     Cody Zeller   C  22  CHO  62  45  1487  172  373  ...  .774   97   \n",
       "674  492    Tyler Zeller   C  25  BOS  82  59  1731  340  619  ...  .823  146   \n",
       "\n",
       "     DRB  TRB  AST  STL BLK  TOV   PF   PTS  \n",
       "0    222  301   68   27  22   60  147   398  \n",
       "1     19   28   16   16   7   14   24    94  \n",
       "2    324  523   66   38  86   99  222   537  \n",
       "3     54   77   15    4   9    9   30    60  \n",
       "4    220  247  129   41   7  116  167  1035  \n",
       "..   ...  ...  ...  ...  ..  ...  ...   ...  \n",
       "670  284  411  173  124  25  117  171  1071  \n",
       "671  170  245  135   86  17   75  115   685  \n",
       "672  114  166   38   38   8   42   56   386  \n",
       "673  265  362  100   34  49   62  156   472  \n",
       "674  319  465  113   18  52   76  205   833  \n",
       "\n",
       "[675 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ##############   ##########  Working with Binary files in Pandas   ###########   ###############\n",
    "    \n",
    " =>  The process of conversion of Object codes into Byte codes is known as Serialisation operation.\n",
    "\n",
    " =>  The process of conversion of Byte codes into object codes is known as De-Serialisation operation.\n",
    "    \n",
    " =>  The combined process of Seriaslisation & Desearialisation is known as Ser-De Operation.\n",
    "\n",
    " =>  pd.read_pickle() is used to create a Dataframe from Byte codes or Pickle files.\n",
    "    \n",
    " =>  df.to_pickle() is used to transform an Object code into Byte codes and store it as a Pickle File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('df.pickle')   # This is Serialization operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rk</th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>FG</th>\n",
       "      <th>FGA</th>\n",
       "      <th>...</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quincy Acy</td>\n",
       "      <td>PF</td>\n",
       "      <td>24</td>\n",
       "      <td>NYK</td>\n",
       "      <td>68</td>\n",
       "      <td>22</td>\n",
       "      <td>1287</td>\n",
       "      <td>152</td>\n",
       "      <td>331</td>\n",
       "      <td>...</td>\n",
       "      <td>.784</td>\n",
       "      <td>79</td>\n",
       "      <td>222</td>\n",
       "      <td>301</td>\n",
       "      <td>68</td>\n",
       "      <td>27</td>\n",
       "      <td>22</td>\n",
       "      <td>60</td>\n",
       "      <td>147</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jordan Adams</td>\n",
       "      <td>SG</td>\n",
       "      <td>20</td>\n",
       "      <td>MEM</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>35</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>.609</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Steven Adams</td>\n",
       "      <td>C</td>\n",
       "      <td>21</td>\n",
       "      <td>OKC</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>1771</td>\n",
       "      <td>217</td>\n",
       "      <td>399</td>\n",
       "      <td>...</td>\n",
       "      <td>.502</td>\n",
       "      <td>199</td>\n",
       "      <td>324</td>\n",
       "      <td>523</td>\n",
       "      <td>66</td>\n",
       "      <td>38</td>\n",
       "      <td>86</td>\n",
       "      <td>99</td>\n",
       "      <td>222</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Jeff Adrien</td>\n",
       "      <td>PF</td>\n",
       "      <td>28</td>\n",
       "      <td>MIN</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>19</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>.579</td>\n",
       "      <td>23</td>\n",
       "      <td>54</td>\n",
       "      <td>77</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Arron Afflalo</td>\n",
       "      <td>SG</td>\n",
       "      <td>29</td>\n",
       "      <td>TOT</td>\n",
       "      <td>78</td>\n",
       "      <td>72</td>\n",
       "      <td>2502</td>\n",
       "      <td>375</td>\n",
       "      <td>884</td>\n",
       "      <td>...</td>\n",
       "      <td>.843</td>\n",
       "      <td>27</td>\n",
       "      <td>220</td>\n",
       "      <td>247</td>\n",
       "      <td>129</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>116</td>\n",
       "      <td>167</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>490</td>\n",
       "      <td>Thaddeus Young</td>\n",
       "      <td>PF</td>\n",
       "      <td>26</td>\n",
       "      <td>TOT</td>\n",
       "      <td>76</td>\n",
       "      <td>68</td>\n",
       "      <td>2434</td>\n",
       "      <td>451</td>\n",
       "      <td>968</td>\n",
       "      <td>...</td>\n",
       "      <td>.655</td>\n",
       "      <td>127</td>\n",
       "      <td>284</td>\n",
       "      <td>411</td>\n",
       "      <td>173</td>\n",
       "      <td>124</td>\n",
       "      <td>25</td>\n",
       "      <td>117</td>\n",
       "      <td>171</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>490</td>\n",
       "      <td>Thaddeus Young</td>\n",
       "      <td>PF</td>\n",
       "      <td>26</td>\n",
       "      <td>MIN</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>1605</td>\n",
       "      <td>289</td>\n",
       "      <td>641</td>\n",
       "      <td>...</td>\n",
       "      <td>.682</td>\n",
       "      <td>75</td>\n",
       "      <td>170</td>\n",
       "      <td>245</td>\n",
       "      <td>135</td>\n",
       "      <td>86</td>\n",
       "      <td>17</td>\n",
       "      <td>75</td>\n",
       "      <td>115</td>\n",
       "      <td>685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>490</td>\n",
       "      <td>Thaddeus Young</td>\n",
       "      <td>PF</td>\n",
       "      <td>26</td>\n",
       "      <td>BRK</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>829</td>\n",
       "      <td>162</td>\n",
       "      <td>327</td>\n",
       "      <td>...</td>\n",
       "      <td>.606</td>\n",
       "      <td>52</td>\n",
       "      <td>114</td>\n",
       "      <td>166</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>56</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>491</td>\n",
       "      <td>Cody Zeller</td>\n",
       "      <td>C</td>\n",
       "      <td>22</td>\n",
       "      <td>CHO</td>\n",
       "      <td>62</td>\n",
       "      <td>45</td>\n",
       "      <td>1487</td>\n",
       "      <td>172</td>\n",
       "      <td>373</td>\n",
       "      <td>...</td>\n",
       "      <td>.774</td>\n",
       "      <td>97</td>\n",
       "      <td>265</td>\n",
       "      <td>362</td>\n",
       "      <td>100</td>\n",
       "      <td>34</td>\n",
       "      <td>49</td>\n",
       "      <td>62</td>\n",
       "      <td>156</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>492</td>\n",
       "      <td>Tyler Zeller</td>\n",
       "      <td>C</td>\n",
       "      <td>25</td>\n",
       "      <td>BOS</td>\n",
       "      <td>82</td>\n",
       "      <td>59</td>\n",
       "      <td>1731</td>\n",
       "      <td>340</td>\n",
       "      <td>619</td>\n",
       "      <td>...</td>\n",
       "      <td>.823</td>\n",
       "      <td>146</td>\n",
       "      <td>319</td>\n",
       "      <td>465</td>\n",
       "      <td>113</td>\n",
       "      <td>18</td>\n",
       "      <td>52</td>\n",
       "      <td>76</td>\n",
       "      <td>205</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>675 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rk          Player Pos Age   Tm   G  GS    MP   FG  FGA  ...   FT%  ORB  \\\n",
       "0      1      Quincy Acy  PF  24  NYK  68  22  1287  152  331  ...  .784   79   \n",
       "1      2    Jordan Adams  SG  20  MEM  30   0   248   35   86  ...  .609    9   \n",
       "2      3    Steven Adams   C  21  OKC  70  67  1771  217  399  ...  .502  199   \n",
       "3      4     Jeff Adrien  PF  28  MIN  17   0   215   19   44  ...  .579   23   \n",
       "4      5   Arron Afflalo  SG  29  TOT  78  72  2502  375  884  ...  .843   27   \n",
       "..   ...             ...  ..  ..  ...  ..  ..   ...  ...  ...  ...   ...  ...   \n",
       "670  490  Thaddeus Young  PF  26  TOT  76  68  2434  451  968  ...  .655  127   \n",
       "671  490  Thaddeus Young  PF  26  MIN  48  48  1605  289  641  ...  .682   75   \n",
       "672  490  Thaddeus Young  PF  26  BRK  28  20   829  162  327  ...  .606   52   \n",
       "673  491     Cody Zeller   C  22  CHO  62  45  1487  172  373  ...  .774   97   \n",
       "674  492    Tyler Zeller   C  25  BOS  82  59  1731  340  619  ...  .823  146   \n",
       "\n",
       "     DRB  TRB  AST  STL BLK  TOV   PF   PTS  \n",
       "0    222  301   68   27  22   60  147   398  \n",
       "1     19   28   16   16   7   14   24    94  \n",
       "2    324  523   66   38  86   99  222   537  \n",
       "3     54   77   15    4   9    9   30    60  \n",
       "4    220  247  129   41   7  116  167  1035  \n",
       "..   ...  ...  ...  ...  ..  ...  ...   ...  \n",
       "670  284  411  173  124  25  117  171  1071  \n",
       "671  170  245  135   86  17   75  115   685  \n",
       "672  114  166   38   38   8   42   56   386  \n",
       "673  265  362  100   34  49   62  156   472  \n",
       "674  319  465  113   18  52   76  205   833  \n",
       "\n",
       "[675 rows x 30 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('df.pickle')     #  This is De-serialisation operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ###############   ############    Working with json data in Pandas    #############    ################\n",
    "    \n",
    " =>  json stands for Javascript Object Notation is an open standard file format that uses human-readable text\n",
    "           to store and transmit Data objects.\n",
    "    \n",
    " =>  json objects behaves similar to a Python dictionary but not a dictionary. \n",
    "\n",
    " =>  json objects can be created using a nested dictionary.\n",
    " \n",
    " =>  json should be imported and then json object can be created using json.loads(dict)\n",
    "    \n",
    " =>  The type of created json object is dictionary.\n",
    "   \n",
    " =>  df.to_json(\"file_name\", orient = \"records\") is used to store similar data in json file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>pets</th>\n",
       "      <th>key1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scott</td>\n",
       "      <td>30</td>\n",
       "      <td>[zeus, zuco]</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>katie</td>\n",
       "      <td>38</td>\n",
       "      <td>[Sixes, stache, cisco]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name age                    pets          key1\n",
       "0  scott  30            [zeus, zuco]  [1, 2, 3, 4]\n",
       "1  katie  38  [Sixes, stache, cisco]           NaN"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Python program to create a json object from Python dictionary\n",
    "\n",
    "# Creating a Nested dictionary\n",
    "dict = \"\"\"\n",
    "         {\"name\":\"wes\", \"places_lived\":[\"USA\", \"Spain\", \"Germany\"], \"pet\":\"null\",\n",
    "          \"Siblings\":[{\"name\":\"scott\", \"age\":\"30\", \"pets\":[\"zeus\", \"zuco\"], \"key1\": [1, 2, 3, 4]},\n",
    "          {\"name\":\"katie\", \"age\":38, \"pets\":[\"Sixes\", \"stache\", \"cisco\"]}]} \"\"\"\n",
    "\n",
    "# Importing json and creating json object\n",
    "import json\n",
    "\n",
    "j = json.loads(dict)\n",
    "\n",
    "# Creating a dataframe with json object\n",
    "df = pd.DataFrame(j[\"Siblings\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " * pd.read_feather(path, columns=None, use_threads: bool = True)\n",
    "  \n",
    " => This loads a feather-format object from the file path and makes a Dataframe.\n",
    "    \n",
    " => “Feather” is a fast, lightweight, language agnostic and easy-to-use binary file format for storing dataframes.\n",
    "\n",
    " =>  $   OR   $ pip install feather-format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Different file formats available in pandas](https://pandas.pydata.org/docs/user_guide/io.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " * pd.read_parquet(path, engine : str = 'auto', columns=None, **kwargs)\n",
    "    \n",
    " => This loads a parquet object from the file path & return a DataFrame.\n",
    "\n",
    " => Apache Parquet is a free and open-source column-oriented data storage format of the Apache Hadoop ecosystem. \n",
    "    \n",
    " => It is similar to the other columnar-storage file formats available in Hadoop namely RCFile and ORC.\n",
    "\n",
    " => It is compatible with most of the data processing frameworks in the Hadoop environment.\n",
    "    \n",
    "\n",
    " * df.to_parquet(path, engine='auto', compression='snappy', index=None, partition_cols=None, **kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0     1     3\n",
       "1     2     4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python program to create & read a dataframe using Parquet Files.\n",
    "df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n",
    "\n",
    "# storing dataframe to Parquet file\n",
    "df.to_parquet('files/parquet_file.gzip', compression='gzip')\n",
    "\n",
    "# Reading dataframe from a Parquet File\n",
    "pd.read_parquet('files/df.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " * pd.read_orc(path, columns=None, **kwargs)\n",
    "    \n",
    "  => This loads a Dataframe from 'orc file', and makes a Dataframe.\n",
    "\n",
    "  => Apache ORC (Optimized Row Columnar) is a free and open-source column-oriented data storage format of \n",
    "       the Apache Hadoop ecosystem. \n",
    "        \n",
    "  => It is similar to the other columnar-storage file formats available in the Hadoop ecosystem such as RCFile \n",
    "       and Parquet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " * pd.read_fwf(filepath, colspecs='infer', widths=None, infer_nrows=100, **kwds)\n",
    "    \n",
    "    =>  This loads a dataframe from FWF (Fixed-Width Formatted lines) file.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [link](https://pypi.org/project/fwf/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ##############   ############   Working with HDF5 Files in Pandas    ##############   ############\n",
    "\n",
    " => Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and \n",
    "     contents of a file with no outside information.\n",
    "        \n",
    " => One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects.\n",
    "    \n",
    " => In order to add another DataFrame or Series to an existing HDF file use append mode and a different key.\n",
    "    \n",
    "\n",
    " * pd.read_hdf(path, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\n",
    "    \n",
    "    =>  This loads a dataframe from hdf files at a given path.\n",
    "    \n",
    " * df.to_hdf(path, key, mode, complevel, complib, append, format, index, min_itemsize, nan_rep, dropna,\n",
    "          data_columns, errors, encoding)\n",
    "  \n",
    "    =>  This writes a combined dataframe to an HDF5 file using HDFStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ##############   ############   Working with gbq Files in Pandas    ##############   ############\n",
    "\n",
    " => pandas-gbq is a package providing an interface to the Google BigQuery API from pandas.\n",
    "\n",
    " => The pandas_gbq module provides a wrapper for Google’s BigQuery analytics web service to simplify \n",
    "     retrieving results from BigQuery tables using SQL-like queries.\n",
    "        \n",
    " => Result sets are parsed into a pandas DataFrame with a shape and data types derived from the source table. \n",
    "      Additionally, DataFrames can be inserted into new BigQuery tables or appended to existing tables.\n",
    "\n",
    " => Install GBQ files -\n",
    "    \n",
    "      $ conda install pandas-gbq --channel conda-forge      &  $ pip install pandas-gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ##############  ############   Some more methods to deal with dataframes in python     ######  #############   \n",
    " \n",
    " * pd.read_sas() -> \n",
    "\n",
    " * pd.read_spss() ->\n",
    "    \n",
    " * pd.read_sql() ->\n",
    "\n",
    " * pd.read-sql_query() ->\n",
    "\n",
    " * pd.read_sql_table() ->\n",
    "\n",
    " * pd.read_stata() ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ############    ############   Working with EXCEL Files In Pandas   ###########   #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ###########    ##########   Loading Excel spreadsheet as Pandas DataFrame    ############   ############\n",
    "    \n",
    " =>  We first need to import Pandas and load excel file, and then parse excel file sheets as a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ###########    ##########  Reading excel files using read_excel() method  ###########    ##############\n",
    "\n",
    "=>  read_excel() methods with 'sheet_name' & 'usecols' attribute is used to get a specific sheet from \n",
    "    \n",
    "       >>> pd.read_excel(file, sheet_name, usecols)\n",
    "        \n",
    "    * 'sheet_name = None' returns all sheets of excel file & '0 or 1' gives specific sheet of excel file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     ############    ############   Working with Excel files in Pandas    ###########    #############\n",
    "    \n",
    "#   ##########   Handling missing data using 'na_values' parameter of the read_excel() method    ###########\n",
    "  \n",
    "df1 = pd.read_excel('files/pandas.xlsx', na_values = \"Missing\", sheet_name = 0) \n",
    "\n",
    "#     ########   Skip starting rows in Excel File using 'skiprows' parameter of read_excel() method   ########\n",
    "\n",
    "df2 = pd.read_excel('files/pandas.xlsx', sheet_name = 0, skiprows = 2)\n",
    "\n",
    "#  Set the header to any row and start reading from that row, using 'header' parameter  of the read_excel() method.\n",
    "\n",
    "df3 = pd.read_excel('sample.xls', sheet_name = 0, header = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       #############   ###########    Loading google Sheets in Pandas    ###############     ###############\n",
    "\n",
    "# Assigning sheet_id and sheet_name\n",
    "sheet_id = \"1-eEosWNqKvCfcB7ytcNCU0u0E0Y-KcbB6Z4CrLWzpTw\"\n",
    "sheet_name = \"Sheet1\"\n",
    "\n",
    "# Creating Sheet_url for creating dataframe\n",
    "sheet_url = \"https://docs.google.com/spreadsheets/d/{}/gviz/tq?tqx=out:csv&sheet={}\".format(sheet_id, sheet_name)\n",
    "\n",
    "#  Creating Pandas dataframe with google sheets file\n",
    "df = pd.read_csv(sheet_url)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ###############     ########    Pandas - Working with DataFrame & Series   ########    ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #############   ###########   Indexing Columns in DataFrame   #############   ############\n",
    "\n",
    " =>  Indexing operator '[]' with reference to DataFrame or df is used to indexing the data of rows or columns.\n",
    "\n",
    " =>  df[\"Column_name\"]  -> This access a single column from dataframe. \n",
    " \n",
    " =>  df[[\"col_1\", \"col_2\", \"col_3\"]]  ->  This access multiple columns by passing a list of their names.\n",
    "    \n",
    " =>  df[df.columns[i:j]]  ->  This access columns in a range (i, j) excluding jth column whereas (i & j) are numerical values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ###########   ###########   Indexing Rows  #############    ##############   \n",
    "    \n",
    " =>  df.loc[\"row_name\"]  ->  This access a single row of dataFrame.\n",
    "    \n",
    " =>  df.loc[[\"row_1\", \"row_2\", \"row+_3\"]]  ->  This access multiple rows from a dataframe.\n",
    "\n",
    " =>  df.loc['ith row' : 'jth row']  ->  This access multiple rows in a range means all rows from ('i'th index to 'j'th index).\n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     #############    ###########   Indexing Rows and Columns  #############   ############\n",
    "        \n",
    "          ##########     ##########    iloc[] method   ############    ##############\n",
    "    \n",
    " =>  iloc[] method is Purely integer-location based indexing for selection by position.\n",
    "\n",
    " =>  df.iloc['row_name']  ->  This access a single specified row from dataframe..\n",
    "    \n",
    " =>  df.iloc[[1, 2, 3, 4]]  -> This access multiple rows by passing a List of their positional index with iloc[] method.\n",
    "\n",
    " =>  df.iloc[i:j]  ->  This access all rows from i to j.\n",
    "    \n",
    " =>  df.iloc[a:b, i:j]  ->  This access rows in range (a,b) and columns (i:j).\n",
    "\n",
    " =>  df.iat[i,j]  ->  This access a particular data of ith row and jth column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ############     ##########    Boolean Indexing in Pandas   ##########    #############\n",
    "    \n",
    " =>  In boolean indexing, We select subsets of data based on the actual values of the data in the DataFrame\n",
    "       and not on their row/column labels or integer locations\n",
    "    \n",
    " =>   We use a boolean vector to filter the data. \n",
    "   \n",
    " =>  In boolean indexing, we can filter a data in following ways.\n",
    "\n",
    "        *  Accessing a DataFrame with a boolean index\n",
    "    \n",
    "        *  Applying a boolean mask to a dataframe\n",
    "\n",
    "        *  Masking data based on column value\n",
    "\n",
    "        *  Masking data based on index value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# dictionary of lists \n",
    "dict = {'name':[\"aparna\", \"pankaj\", \"sudhir\", \"Geeku\"], \n",
    "\t\t'degree': [\"MBA\", \"BCA\", \"M.Tech\", \"MBA\"], \n",
    "\t\t'score':[90, 40, 80, 98]} \n",
    "\n",
    "# creating a dataframe with boolean index \n",
    "df = pd.DataFrame(dict, index = [True, False, True, False]) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ##########    ##########    Accessing a DataFrame with a boolean index    ##########    ##############\n",
    "\n",
    " =>  In order to access a dataframe with a boolean index, we have to create a dataframe in which \n",
    "        index of dataframe contains a boolean value that is “True” or “False”. \n",
    "    \n",
    " =>  df.loc() is used to access dataframe with Boolean indexes.\n",
    "\n",
    " =>  df.loc[index_name]  ->  This access all rows associated with this index_name.\n",
    "    \n",
    "        >>> df.loc[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a dataframe using .loc[] function \n",
    "\n",
    "df.loc[True] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ############     ############   Filtering a dataset using Boolean indexing    ############    #############\n",
    "\n",
    " =>  In a dataframe we can filter a data based on a column value in order to filter data, we can apply certain condition \n",
    "       on dataframe using different operator like ==, >, <, <=, >=.\n",
    "    \n",
    " =>  When we apply these operator on dataframe then it produce a Series of True and False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('files/file.csv').head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a comparsion operator for filtering of data \n",
    "df['Salary'] >= 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Masking data based on Index values     \n",
    "\n",
    "mask = df.index >= 0\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ###########    ############   Missing Values in Pandas    ############    #############\n",
    "\n",
    " =>  Missing Data can occur when no information is provided for one or more items or for a whole unit.\n",
    "\n",
    " =>  Types of missing values in Python :\n",
    "        \n",
    "     * None: None is a Python singleton object that is often used for missing data in Python code.\n",
    "\n",
    "     * NaN (Not a Number) is a special floating-point value recognized by all systems that uses Standard IEEE floating-point\n",
    "                                                                                                             representation.\n",
    "    \n",
    "    \n",
    "    #########    ##########   Checking Missing values in Data Frame   ##########    ############\n",
    "\n",
    " =>  df.isnull() and df.notnull() returns a dataframe with boolean values after checking the Nan values.\n",
    "\n",
    "      >>>  df.isnull()      or    >>>  df.notnull()\n",
    "    \n",
    "    \n",
    "     ###########   ############   Filling Missing Values in DataFrame    ###########    ############\n",
    "    \n",
    " =>   =>  df.dropna() is used to drop or removethe rows or columns with Nan values from DataFrame.\n",
    "    \n",
    "       >>>  df.dropna(axis, how='any', thresh=None, subset=None, inplace=False)\n",
    "        \n",
    "        \n",
    "          * axis(0 or 1)  ->  This determine the rows or columns having Missing values to be removed.\n",
    "            \n",
    "          * how ('any', 'all')  ->  'any' says to drop the row or column if they have any NA values and \n",
    "                                    'all' says to drop the row or column if all values are NA.\n",
    "    \n",
    "          * subset (array-like, optional)  ->  This Labels along other axis to consider, e.g. if you are dropping rows these \n",
    "                                                   would be a list of columns to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ###########   ############   Filling Missing Values in DataFrame    ###########    ############\n",
    "\n",
    " =>  df.fillna() is used to fill a specified value instead of Nan values in dataFrame.\n",
    "\n",
    "        >>>  df.fillna(value, method, axis, inplace, limit, downcast)\n",
    "       \n",
    "        * method ('backfill', 'bfill', 'pad', 'ffil', None)  ->  This specifies a method to fill the values.\n",
    "    \n",
    " >>> df.fillna(value)  ->  This fills a specified value on place of NA values.\n",
    "\n",
    " >>> df.fillna(method = 'pad')  ->  This fills by padding with previous values.\n",
    " \n",
    " >>> df.fillna(method = 'backfill')  ->  This fills by padding with next values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  df.replace() is used to replace a previous value with a new value.\n",
    "\n",
    "        >>> df.replace(to_replace, value, inplace=False, limit=None, regex=False, method='pad')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  df.interpolate() is used to Interpolate values according to different methods.\n",
    "\n",
    "      >>> df.interpolate(method='linear', axis=0, limit=None, inplace=False, limit_direction='forward',\n",
    "                                                                         limit_area=None, downcast=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ##############    ##########    Iterating over Pandas Dataframe    ###########    ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #########   ########   iterrows() and iteritems()   ##########    ##########\n",
    "    \n",
    " =>  df.iterrows() is used to iter thru the rows of a dataframe.\n",
    "\n",
    "     >>> for i, j in data.iterrows(): \n",
    "           print(i, j) \n",
    "           print()\n",
    "\n",
    " =>  df.iteritems() iterates over each column as key, value pair with label as key \n",
    "                      and column value as a Series object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ############   #########   iterating with itertuples()   ##########  ###########\n",
    "    \n",
    " => df.tertuples()  return a tuple for each row in the DataFrame. \n",
    "\n",
    " => The first element of the tuple will be the row’s corresponding index value,\n",
    "        while the remaining values are the row values.\n",
    "        \n",
    "    >>> for i in data.itertuples(): \n",
    "             print(i, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ###########    ########   Iterating over columns   ###########    ############\n",
    "    \n",
    " =>  In order to iterate over columns, we need to create a list of dataframe columns \n",
    "         and then iterating through that list to pull out the dataframe columns.\n",
    "    \n",
    " =>  We first create a list of dataframe columns and then iterate through list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of dataframe columns \n",
    "columns = list(df) \n",
    "\n",
    "for i in columns: \n",
    "    \n",
    "    # printing the third element of the column \n",
    "    print (df[i]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ############   ##########    Data Manipulation  on Dataframe   ########    ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##############   ##########  Converting DataFrame to Numpy nd-array  ##########   #############\n",
    "\n",
    " =>  Pandas DataFrame can be converted to NumPy ndarray with the help of Dataframe.to_numpy() method.\n",
    "\n",
    " =>  df.to_numpy(dtype=None, copy=False) -> numpy.ndarray\n",
    "    \n",
    "     *  dtype: Data type which we are passing like str.\n",
    "\n",
    "     *  copy: [bool, default False] Ensures that the returned value is a not a view on another array.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  ##############    #########    Converting  Pandas Series to a Numpy array   #########    #############\n",
    "    \n",
    " =>  Pandas Series.to_numpy() function is used to return a NumPy ndarray representing the values\n",
    "          in given Series or Index.\n",
    "\n",
    "    Syntax: Series.to_numpy()\n",
    "        \n",
    "        *  dtype: Data type which we are passing like str.\n",
    "   \n",
    "        *  copy : [bool, default False] Ensures that the returned value is a not a view on another array.\n",
    "        \n",
    " =>  Changing the Series into numpy array by using a method Series.to_numpy(). \n",
    "      Always remember that when dealing with lot of data you should clean the data first to get the high accuracy. \n",
    "    Although in this code we use the first five values of Weight column by using .head() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ##############    #######  Adding new column to existing DataFrame in Pandas    ###########   #############\n",
    "    \n",
    " =>  There are various ways to add a new column to an Existing DataFrame.\n",
    "\n",
    "       * By declaring a new list as a column and assign to dataframe  ->   >>> df['column_name'] = list\n",
    "       \n",
    "       * By using df.insert()        ->   >>>  df.insert(location, column_name, value, allow_duplicates=False)\n",
    "         \n",
    "       * Using df.assign() method   ->   >>>  df = df.assign(column_name = [Value1, value2, ...])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column to dataframe using various ways\n",
    "import pandas as pd \n",
    "\n",
    "# Define a dictionary containing Students data \n",
    "data = {'Name': ['Jai', 'Princi', 'Gaurav', 'Anuj'], \n",
    "\t\t'Height': [5.1, 6.2, 5.1, 5.2], \n",
    "\t\t'Qualification': ['Msc', 'MA', 'Msc', 'Msc']} \n",
    "\n",
    "# Convert the dictionary into DataFrame \n",
    "df = pd.DataFrame(data) \n",
    "\n",
    "# Using 'Address' as the column name and equating it to the list \n",
    "df['Address'] = ['Delhi', 'Bangalore', 'Chennai', 'Patna']\n",
    "\n",
    "# Adding a new column using df.insert()\n",
    "df.insert(2, \"Age\", [21, 23, 24, 21], True) \n",
    "\n",
    "#  Adding a new column to dataframe using df.assign() method\n",
    "df = df.assign(Price = [250,800,1200,300])\n",
    "\n",
    "# Observe the result \n",
    "df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #########   ##########   Deleting or Removing Rows or columns from a dataframe  ##########   #########\n",
    "\n",
    " =>  df.drop() method is used to delete or remove rows or columns from a dataframe.\n",
    "    \n",
    " =>  Rows or columns can be removed using index label or column name using this method.\n",
    "\n",
    "   =>  df.drop( labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')\n",
    "\n",
    "      * labels: String or list of strings referring row or column name.\n",
    "    \n",
    "      * axis: int or string value, '0' for Rows and '1' for Columns.\n",
    "\n",
    "      * index or columns (Single label or list) index or columns are an alternative to axis and cannot be used together.\n",
    "\n",
    "      * level: Used to specify level in case data frame is having multiple level index.\n",
    "\n",
    "      * inplace: Makes changes in original Data Frame if True.\n",
    "\n",
    "      * errors: Ignores error if any value from the list doesn’t exists and drops rest of the values when errors = ‘ignore’\n",
    "\n",
    "      * Return type: Dataframe with dropped values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  Pandas DataFrame.truncate() function is used to truncate a Series or DataFrame before and after some index value. \n",
    "\n",
    " =>  This is a useful shorthand for boolean indexing based on index values above or below certain thresholds.\n",
    "\n",
    "   =>  Syntax: df.truncate(before=None, after=None, axis=None, copy=True)\n",
    "\n",
    "       * before : Truncate all rows before this index value.\n",
    "\n",
    "       * after : Truncate all rows after this index value.\n",
    "\n",
    "       * axis : Axis to truncate. Truncates the index (rows) by default.\n",
    "\n",
    "       * copy : Return a copy of the truncated section.\n",
    "\n",
    "       * Returns : The truncated Series or DataFrame.\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Truncate a dataframe](https://www.geeksforgeeks.org/python-pandas-dataframe-truncate/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ##########    ##########   Sorting a Dataframe in Pandas  with single Parameter ###########    ############\n",
    "    \n",
    " =>  Pandas sort_values() function sorts a data frame in Ascending or Descending order of passed Column.\n",
    "\n",
    "     =>  DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind=’quicksort’, na_position=’last’)\n",
    "    \n",
    "          * by: Single/List of column names to sort Data Frame by.\n",
    "\n",
    "          * axis: 0 or ‘index’ for rows and 1 or ‘columns’ for Column.\n",
    "\n",
    "          * ascending: Boolean value which sorts Data frame in ascending order if True.\n",
    "\n",
    "          * inplace: Boolean value. Makes the changes in passed data frame itself if True.\n",
    "\n",
    "          * kind: String which can have three inputs(‘quicksort’, ‘mergesort’ or ‘heapsort’) of algorithm used to sort \n",
    "                     data frame.\n",
    "\n",
    "          * na_position: Takes two string input ‘last’ or ‘first’ to set position of Null values. default is ‘last’.\n",
    "                \n",
    "          * Returns a sorted Data Frame with Same dimensions as of the function caller Data Frame.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# making data frame from csv file \n",
    "data = pd.read_csv(\"files/file.csv\") \n",
    "\n",
    "# sorting data frame by name \n",
    "data.sort_values(\"Name\", axis = 0, ascending = True, \n",
    "\t\t\t\tinplace = True, na_position ='last') \n",
    "\n",
    "# display \n",
    "data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ###########   #########   Changing Position of Null Values    ###########    ##############\n",
    "\n",
    " =>  In the give data, there are many null values in different columns which are put in the last by default.\n",
    "\n",
    " =>  In this example, the Data Frame is sorted with respect to Salary column and Null values are kept at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas package \n",
    "import pandas as pd \n",
    "\n",
    "# making data frame from csv file \n",
    "data = pd.read_csv(\"file.csv\") \n",
    "\n",
    "# sorting data frame by name \n",
    "data.sort_values(\"Salary\", axis = 0, ascending = True, \n",
    "\t\t\t\tinplace = True, na_position ='first') \n",
    "\n",
    "data \n",
    "# display \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     ##########    ##########   Sorting a Dataframe in Pandas with Multiple Parameter    ###########    ############\n",
    "   \n",
    " =>  DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind=’quicksort’, na_position=’last’)\n",
    " \n",
    " =>  Sorting by Name and Team\n",
    "I\n",
    " =>  A data frame is loaded from the csv file and the data frame is sorted in ascending order of Team and in every Team \n",
    "      the Name is also sorted in Ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pandas package \n",
    "import pandas as pd \n",
    "\n",
    "#making data frame from csv file \n",
    "data=pd.read_csv(\"file.csv\") \n",
    "\n",
    "#sorting data frame by Team and then By names \n",
    "data.sort_values([\"Team\", \"Name\"], axis=0, ascending=True, inplace=True) \n",
    "\n",
    "#display \n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       ############    ##########   Passing list to Ascending Parameter   ##########   ##############\n",
    "    \n",
    " =>  We can also pass a list to the ‘ascending’ Parameter to tell pandas which column to sort how.\n",
    "\n",
    " =>  The index of Boolean in ‘ascending’ parameter should be same as the index of column name in ‘by’ Parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ##############    ##########   Sorting using 3 Columns   ##########   ##############\n",
    "    \n",
    " =>  In the following example, the same Data Frame is sorted by Team name. \n",
    "\n",
    " =>  For every Team, the Data frame is sorted by Age and for every same Age the Data frame is sorted by Height. \n",
    "    \n",
    " =>  This example will explain how multiparameter sorting works in Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pandas package \n",
    "import pandas as pd \n",
    "\n",
    "#making data frame from csv file \n",
    "data=pd.read_csv(\"file.csv\") \n",
    "\n",
    "#sorting data frame by Team, age and height \n",
    "data.sort_values([\"Team\", \"Age\", \"Height\"], axis=0, \n",
    "\t\t\t\tascending=[False,True,False], \n",
    "inplace=True) \n",
    "\n",
    "#display \n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "=>  As shown in the Image, The team name is sorted first, then the Age and for every age, the height is sorted. \n",
    "    \n",
    " =>  In the Team “Washington Wizards” there are 3 Players with age 30. Those 3 people are sorted by their Height \n",
    "         in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "    \n",
    " =>  Pandas head(n=5) method is used to return top n (5 by default) rows of a data frame or series.\n",
    " \n",
    " =>  Pandas tail(n=5) method is used to return n rows from bottom in a dataframe or series.\n",
    "\n",
    " \n",
    " =>  Pandas describe() generates Descriptive statistics include those that summarize the central\n",
    "        \n",
    "            tendency, dispersion and shape of a dataset's distribution, excluding ``NaN`` values.\n",
    "        \n",
    "\n",
    " =>  df.describe(percentiles=None, include=None, exclude=None) -> FrameOrSeries\n",
    "\n",
    "    * percentile ->  This is a list like numbers. By default [.25, .5, .75]. All falls b/w 0 & 1.\n",
    "\n",
    "    * include: List of data types to be included while describing dataframe. Default is None\n",
    "\n",
    "    * exclude: List of data types to be Excluded while describing dataframe. Default is None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ###############   ##########    Some Popular Methods in Pandas    #########    ############### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  df.head(n)  ->  This return top 'n' rows of dataframe or series. By default n = 5. \n",
    " =>  df.tail(n)  ->  This return 'n' rows from bottom of dataframe or series. By default n = 5\n",
    "\n",
    " =>  df.index    ->  This gives information about the index of dataframe or series.\n",
    " =>  df.columns  ->   This return all columns of a dataframe.\n",
    " =>  df.dtypes   ->  This return dtypes of each column of dataframe.\n",
    " =>  df.astype() ->  This cast a pandas dataframe to given dtype. \n",
    " =>  df.values   ->  This return  values of all Numerical and Categorical variables of a dataframe.\n",
    " =>  df.shape    ->  This return the shape of dataframe in form of a tuple.\n",
    " =>  df.ndim     ->  This return the total number of dimensions of a dataframe.\n",
    "\n",
    " \n",
    " => df.add(df1, axis)  ->  This adds the values of 2 dataframes according to given axis.\n",
    " => sr.add(sr, axis)   ->  This adds the values of two series. add(), sub(), mul(), div() works in a similar way.\n",
    " \n",
    " => df.prod() & sr.prod()  ->  This gives product for required axis\n",
    "    \n",
    " =>  df.copy(deep=bool)  ->  this makes a Deep or shallow copies of dataframes.\n",
    "\n",
    " =>  df.drop_duplicates()  ->  This return the dataframe with dropping the rows with duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  df.insert(loc, 'column_name', value, allow_duplicates=False)  ->  This inserts a column to a dataframe.\n",
    "\n",
    " =>  df.unique(axis = 0, dropna=True)  ->  This counts number of different observations of an item over a required axis.\n",
    " \n",
    " =>  df.isin([x, y, z, ..])  -> This return a dataframe with Boolean values by checking if  an item is from [x, y, x, ..]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " => df.sort_values(by = str or list of str)  ->  This is used for sorting by given by = (column_names) and axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " => df.pop(item)  ->  Tris pops and return an item (column) from dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " * df.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None) \n",
    "    \n",
    "     ->  This Return a random sample of items from an axis of object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " * df.nsmallest(n, columns, keep='first')  ->  This return the first 'n' rows from columns in Ascendomg order. \n",
    "\n",
    " * df.nlargest(n, columns, keep='first')   ->  This return the n rows from columns in Descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  * df.where(condn, other=nan, inplace=False, axis=None, level=None, errors='raise', try_cast=False)\n",
    "\n",
    " => This replace values, where the condition is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  * df.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False)\n",
    "    \n",
    "    =>  Set the DataFrame index using existing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         ##############   ###########   Some popular methods for Pandas Series    #############   ###############\n",
    "    \n",
    " =>  sr.pow(n, level=None, fill_value=None)  ->  this computes the exponent of each items of a series.\n",
    "\n",
    " =>  sr.mean()  ->  This gives the Mean value of series.\n",
    "    \n",
    " =>  sr.abs()   ->  This return absolute numerical values for a dataframe or a series.\n",
    "\n",
    " =>  sr.cov(other)  ->  This return covariances of series.\n",
    "\n",
    " =>  sr.count(level=None)  ->  This return non-NA observations or no. of values in a series.\n",
    "\n",
    " =>  sr.size               ->  This return the no. of values or items of a series.\n",
    "\n",
    " =>  sr.name               ->  This return the na,me of Series.\n",
    "\n",
    " =>  sr.is_unique          ->  This checks for if items of series are unique or not and return a Bool value.\n",
    " \n",
    " =>  sr.sort_values()      ->  This is used to sort the values of a series.\n",
    "\n",
    " =>  sr.le(n)              ->  This checks that values of series is leass than or equal to 'n' and then return Boolean series.\n",
    "\n",
    " =>  sr.ne(n)              ->  This checks for all items not equal to 'n' .\n",
    "    \n",
    " =>  sr.ge(n)              ->  This checks for all items greater than or equal to 'n' .\n",
    "\n",
    " =>  sr.eq(n)              ->  This checks for all items of series are equal to 'n' or not.\n",
    "    \n",
    " =>  sr.gt() &  sr.lt()    ->  These checks for Grater than & Less than conditions for all items of a series.\n",
    "\n",
    " =>  sr.unique()           ->  This return unique values of Series object.\n",
    "\n",
    " =>  sr.nunique()          ->  This return number of unique elements in the series.\n",
    "\n",
    " =>  sr.value_counts()     ->  This return a series of counts of unique values.\n",
    "\n",
    " =>  sr.factorize()        ->  This Encode the object as an enumerated type or categorical variable.\n",
    "    \n",
    " =>  sr.apply(func)        ->  This is used to combine a function with series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ############      ################   Grouping operation In Pandas    ###############    #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "             ###########    ##########   Grouping data in Pandas   ##########   #############\n",
    "    \n",
    " =>  We can create a grouping of categories and apply a function to the categories.\n",
    "\n",
    " =>  Groupby mainly refers to a process involving one or more of the following steps they are:\n",
    "\n",
    "    *  Splitting : It is a process in which we split data into group by applying some conditions on datasets.\n",
    "\n",
    "    *  Applying  : It is a process in which we apply a function to each group independently\n",
    "\n",
    "    *  Combining : It is a process in which we combine different datasets after applying groupby and results \n",
    "                     into a data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              ###########    ##########   Splitting Data into groups    ##########   #############\n",
    "  \n",
    " =>  Splitting is a process in which we split data into a group by applying some conditions on datasets.\n",
    "      \n",
    " =>  groupby() function  is used to split the data into groups based on some criteria.\n",
    "        \n",
    " =>  Pandas objects can be split on any of their axes. \n",
    "  \n",
    " =>  The abstract definition of grouping is to provide a mapping of labels to group names.\n",
    " \n",
    " =>  Pandas datasets can be split into any of their objects. There are multiple ways to split data like:\n",
    "\n",
    "         *  obj.groupby(key)\n",
    "\n",
    "         *  obj.groupby(key, axis=1)\n",
    "\n",
    "         *  obj.groupby([key1, key2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "             ################     ##############   Groupby()  method     ###############    ################\n",
    "    \n",
    " =>  groupby() method is used to performing grouping operations over dataframes using a mapper or by a Series of columns.\n",
    "\n",
    " =>  Applying Group-by() gives an object and then we can directly perform all basic data manipulations on top of it. \n",
    "    \n",
    " =>  groupby objects can be printed using first() method.\n",
    "\n",
    "     >>> df.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, sqyeeze=False, observed=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           ###########   #########  Grouping data with one key   ###########    ############\n",
    "\n",
    " =>  In order to group data with one key, we pass only one key as an argument in groupby() function.\n",
    "\n",
    "        >>>  group = df.groupby('Name') \n",
    "        >>>  group.first()                             # Getting 1st entries of grouped objects\n",
    "\n",
    "        \n",
    "           ###########   #########  Grouping data with multiple keys   ###########    ############\n",
    "    \n",
    " =>  In order to group data with multiple keys, we pass multiple keys in groupby function.\n",
    "\n",
    "        >>>  group = df.groupby(['Name', 'Qualification'])\n",
    "        >>>  group.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           #############   Python program to show the groupingg of datasets using groupby() method    ############\n",
    "\n",
    "# importing pandas module \n",
    "import pandas as pd \n",
    "# importing numpy as np \n",
    "import numpy as np \n",
    "\n",
    "# Define a dictionary containing employee data \n",
    "data1 = {'Name':['Jai', 'Anuj', 'Jai', 'Princi', \n",
    "\t\t\t\t'Gaurav', 'Anuj', 'Princi', 'Abhi'], \n",
    "\t\t'Age':[27, 24, 22, 32, \n",
    "\t\t\t33, 36, 27, 32], \n",
    "\t\t'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj', \n",
    "\t\t\t\t'Jaunpur', 'Kanpur', 'Allahabad', 'Aligarh'], \n",
    "\t\t'Qualification':['Msc', 'MA', 'MCA', 'Phd', \n",
    "\t\t\t\t\t\t'B.Tech', 'B.com', 'Msc', 'MA']} \n",
    "\t\n",
    "\n",
    "# Convert the dictionary into DataFrame \n",
    "df = pd.DataFrame(data1) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##############    ##############   Splitting dataset using one key & multiple keys    ###############     #############\n",
    "\n",
    "# Applying groupby() function to group the data on 'Name'  value. \n",
    "group1 = df.groupby('Name') \n",
    "\n",
    "# Applying groupby() function to group the data on 'Name'  value & 'Qualification' value\n",
    "group2 = df.groupby(['Name', 'Qualification'])\n",
    "\n",
    "\n",
    "# Printing the first entries in all the groups formed. \n",
    "print(group1.first())\n",
    "group2.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ###########   #########  Grouping data by sorting keys   ###########    ############\n",
    "    \n",
    " =>  Group keys are sorted by default using the groupby operation. User can pass sort=False for potential speedups.\n",
    "\n",
    "         >>> df.groupby(['key'], sort = False).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sort=False for fast computations in grouping operations\n",
    "\n",
    "df.groupby(['Name'], sort = False).sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         #############   ##########   Itterating Through groups    ###########   #############\n",
    "    \n",
    " =>  In order to iterate an element of groups, we can iterate through the object similar to itertools.obj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating an element of group \n",
    "\n",
    "grp1 = df.groupby('Name')                        #  Grouping using single key\n",
    "grp2 = df.groupby(['Name', 'Qualification'])     #  Grouping using multiple keys\n",
    "\n",
    "#  Iterating in group\n",
    "for name, group in grp2: \n",
    "\tprint(name) \n",
    "\tprint(group) \n",
    "\tprint() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ###############    #############   Selection of a group   #############    ################\n",
    "    \n",
    " =>  GroupBy.get_group() is used to select a group obtained by Grouping operations.\n",
    "    \n",
    "       >>>  group.get_group('group_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a group\n",
    "grp1.get_group('Jai')                #  Grouped with single key\n",
    "\n",
    "grp2.get_group(('Jai', 'Msc'))       #  Grouped with multiple keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "             ##########    ##########    Applying Functions to group   ##########    ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " => After splitting adataset into groups, we just apply some operations (functions) over them.\n",
    "    \n",
    "    \n",
    "      *  Aggregation    : It is a process in which we compute a summary statistic about each group. \n",
    "                            For Example, Compute group sums or means.\n",
    "            \n",
    "      *  Transformation : It is a process in which we perform some group-specific computations and return a like-indexed.  \n",
    "                            For Example, Filling NAs within groups with a value derived from each group\n",
    "            \n",
    "      *  Filtration     : It is a process in which we discard some groups, according to a group-wise computation that evaluates\n",
    "                            True or False.  \n",
    "                          \n",
    "                            For Example, Filtering out data based on the group sum or mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         ##########    ##########    Aggregation function  ##########    ############\n",
    "\n",
    " =>  Aggregation is a process in which we compute a summary statistic about each group  returns a single aggregated value\n",
    "          for each group. \n",
    "    \n",
    " =>  After splitting a data into groups using groupby function, several aggregation operations can be performed\n",
    "           on the grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          #############    ############   aggregate() method     ############    ############\n",
    " \n",
    " =>  group.aggregate(func)  ->  This is used to peform aggregations by passing one or more functions together.\n",
    "    \n",
    " =>  'func' parameter takes a function, string name of a funcion, list of functions or their names.\n",
    "            \n",
    " =>  series.agg() is called with a single function return a scalar value.\n",
    " \n",
    " =>  dataframe.agg() is called with single function return a series.\n",
    "\n",
    " =>  dataframe.agg() is called with several functions return a dataframe.\n",
    "\n",
    "      >>> group.aggregate(func)                  ->    This is calling a single function.\n",
    "      >>> group.aggregate([func1, func2, ..])    ->    This is calling several functions.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing aggregation using aggregate() method \n",
    "grp1 = df.groupby(['Name', 'Qualification']) \n",
    "\n",
    "grp1.aggregate([np.sum, np.mean, min, max]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  In order to apply a different aggregation functions to the columns of a DataFrame, we can pass a dictionary to aggregate .\n",
    "\n",
    "        >>>  group.agg({'column_1' : 'sum', 'column_2' : 'std'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Different aggregation function on different columns\n",
    "\n",
    "# Define a dictionary containing employee data \n",
    "data1 = {'Name':['Jai', 'Anuj', 'Jai', 'Princi', \n",
    "\t\t\t\t'Gaurav', 'Anuj', 'Princi', 'Abhi'], \n",
    "\t\t'Age':[27, 24, 22, 32, \n",
    "\t\t\t33, 36, 27, 32], \n",
    "\t\t'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj', \n",
    "\t\t\t\t'Jaunpur', 'Kanpur', 'Allahabad', 'Aligarh'], \n",
    "\t\t'Qualification':['Msc', 'MA', 'MCA', 'Phd', \n",
    "\t\t\t\t\t\t'B.Tech', 'B.com', 'Msc', 'MA'], \n",
    "\t\t'Score': [23, 34, 35, 45, 47, 50, 52, 53]} \n",
    "\t\n",
    "\n",
    "# Convert the dictionary into DataFrame \n",
    "df = pd.DataFrame(data1) \n",
    "grp = df.groupby('Name') \n",
    "\n",
    "# Applying different aggregation on different columns\n",
    "grp.agg({'Age' : 'sum', 'Score' : 'std'}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ##########    ##########    Transformation function  ##########    ############\n",
    "\n",
    " =>  Transformation is a process in which we perform some group-specific computations and return a like-indexed. \n",
    "\n",
    " =>  Transform method returns an object that is indexed the same (same size) as the one being grouped.  \n",
    "    \n",
    " =>  The transform function must:\n",
    "\n",
    "          * Return a result that is either the same size as the group chunk\n",
    "          * Operate column-by-column on the group chunk\n",
    "          * Not perform in-place operations on the group chunk.\n",
    "        \n",
    " =>  group.transform(func, axis=0,)  ->  This is used to apply a function on grouping ing objects for transformation. \n",
    "    \n",
    "        >>>  func = lambda x: (x - x.mean()) / x.std()*10\n",
    "        >>>  group.transform(func) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Python code to show transformation of groups\n",
    "grp = df.groupby('Name') \n",
    "\n",
    "# Performing transformations\n",
    "func = lambda x: (x - x.mean()) / x.std()*10\n",
    "grp.transform(func) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ##########    ##########    Filteration  function  ##########    ############\n",
    "\n",
    " =>  Filtration is a process in which we discard some groups, according to a group-wise computation that evaluates either\n",
    "        True or False.  \n",
    "    \n",
    " =>  In order to filter a group, we use filter method and apply some conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Filteration operation over groups \n",
    "grp = df.groupby('Name') \n",
    "\n",
    "# filter data that to return the Name which have lived two or more times .\n",
    "grp.filter(lambda x: len(x) >= 2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        ################     ##########   Grouping Rows in pandas    ############    ###############\n",
    "    \n",
    " =>  Create a dataframe first.\n",
    " \n",
    " =>  Now, create a grouping object, means an object that represents that particular grouping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Grouping rows in Pandas\n",
    " \n",
    "#  Creating a dataframe\n",
    "example = {'Team':['Arsenal', 'Manchester United', 'Arsenal', \n",
    "\t\t\t\t'Arsenal', 'Chelsea', 'Manchester United', \n",
    "\t\t\t\t'Manchester United', 'Chelsea', 'Chelsea', 'Chelsea'], \n",
    "\t\t\t\t\t\n",
    "\t\t'Player':['Ozil', 'Pogba', 'Lucas', 'Aubameyang', \n",
    "\t\t\t\t\t'Hazard', 'Mata', 'Lukaku', 'Morata', \n",
    "\t\t\t\t\t\t\t\t\t\t'Giroud', 'Kante'], \n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t'Goals':[5, 3, 6, 4, 9, 2, 0, 5, 2, 3] } \n",
    "\n",
    "df = pd.DataFrame(example) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a grouping object, means an object that represents that particular grouping.\n",
    "total_goals = df['Goals'].groupby(df['Team']) \n",
    "\n",
    "# Printing the means value \n",
    "print(total_goals.mean()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       ###############   ##########   Merging, Joining & Concatenating dataframes    #############    ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ############    ##########  Concatenating DataFrame    ############    ##########\n",
    "\n",
    " =>  In order to concatenate a dataframe, we use concat() function which helps in concatenating a dataframe. \n",
    "       We can concat a dataframe in many different ways, they are:\n",
    "\n",
    "        * Concatenating DataFrame using .concat()\n",
    "\n",
    "        * Concatenating DataFrame by setting logic on axes\n",
    "\n",
    "        * Concatenating DataFrame using .append()\n",
    "\n",
    "        * Concatenating DataFrame by ignoring indexes\n",
    "\n",
    "        * Concatenating DataFrame with group keys\n",
    "        \n",
    "        * Concatenating with mixed ndims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       ############   ######   Concatenating DataFrame using .concat()  ###########    #############\n",
    "\n",
    " =>  In order to concat a dataframe, we use .concat() function this function concat a dataframe and returns a new dataframe.\n",
    "    \n",
    " =>  After concatenation, 2 different dataframes become a single dataframe.\n",
    "\n",
    " =>  .concat(object, axis=0, join='outer', ignore_index=False, keys=None, levels=None, names=None, sort=False, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing employee data \n",
    "data1 = {'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'], \n",
    "        'Age':[27, 24, 22, 32], \n",
    "        'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Msc', 'MA', 'MCA', 'Phd']} \n",
    "\n",
    "# Creating a dataframe with default index\n",
    "df = pd.DataFrame(data1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing employee data \n",
    "data2 = {'Name':['Abhi', 'Ayushi', 'Dhiraj', 'Hitesh'], \n",
    "        'Age':[17, 14, 12, 52], \n",
    "        'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Btech', 'B.A', 'Bcom', 'B.hons']} \n",
    "\n",
    "# Convert the dictionary into DataFrame with defined index  \n",
    "df1 = pd.DataFrame(data2, index=[4, 5, 6, 7])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now we apply .concat function in order to concat two dataframe \n",
    "\n",
    "# Creating a list with both dataframes\n",
    "list = [df, df1]   \n",
    "\n",
    "# Concatenating datafames by passing the list with concat() method\n",
    "concat_df = pd.concat(list)\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       #############   ############   Concatenating DataFrame by setting logic on axes   ##########    ############\n",
    "    \n",
    " =>  In order to concat dataframe, we have to set different logic on axes. We can set axes in the following\n",
    "        three ways:\n",
    "\n",
    "  *  Taking the intersection, join='inner'.\n",
    "\n",
    "         >>> pd.concat([df, df1], axis=1, join='inner')\n",
    "\n",
    "  *  Taking the union of them all, join='outer' (zero information loss). \n",
    "\n",
    "         >>> pd.concat([df. df1], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing employee data \n",
    "data1 = {'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'], \n",
    "        'Age':[27, 24, 22, 32], \n",
    "        'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Msc', 'MA', 'MCA', 'Phd'],\n",
    "        'Mobile No': [97, 91, 58, 76]} \n",
    "  \n",
    "# Define a dictionary containing employee data \n",
    "data2 = {'Name':['Gaurav', 'Anuj', 'Dhiraj', 'Hitesh'], \n",
    "        'Age':[22, 32, 12, 52], \n",
    "        'Address':['Allahabad', 'Kannuaj', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['MCA', 'Phd', 'Bcom', 'B.hons'],\n",
    "        'Salary':[1000, 2000, 3000, 4000]} \n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df = pd.DataFrame(data1,index=[0, 1, 2, 3])\n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data2, index=[2, 3, 6, 7]) \n",
    "\n",
    "print(df, \"\\n\\n\", df1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we set axes join = inner for intersection of dataframe\n",
    "\n",
    "# applying concat with axes join = 'inner'\n",
    "concat_df = pd.concat([df, df1], axis=1, join='inner')\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using .concat for union of dataframe\n",
    "concat_df = pd.concat([df, df1], axis=1, join='outer')\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  ##########    ##########   Concatenating DataFrame using .append()   #########    #############\n",
    "\n",
    " => append() method is used for concatenating 2 dataframes and is similar to concatenation with axis = 0.\n",
    "    \n",
    "       >>> df.append(df1, ignore_index=False, verify_integrity=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply .append() function inorder to concat to dataframe\n",
    "appended_df = df.append(df1)\n",
    "appended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############    ##########   Concatenating DataFrame by ignoring indexes    ###########    ##############\n",
    "\n",
    " =>  In order to concat a dataframe by ignoring indexes, we ignore index which don’t have a meaningful meaning,\n",
    "        you may wish to append them and ignore the fact that theymay have overlapping indexes.  \n",
    "        \n",
    " =>  In order to do that we use ignore_index as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module\n",
    "import pandas as pd \n",
    " \n",
    "# Define a dictionary containing employee data \n",
    "data1 = {'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'], \n",
    "        'Age':[27, 24, 22, 32], \n",
    "        'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Msc', 'MA', 'MCA', 'Phd'],\n",
    "        'Mobile No': [97, 91, 58, 76]} \n",
    "   \n",
    "# Define a dictionary containing employee data \n",
    "data2 = {'Name':['Gaurav', 'Anuj', 'Dhiraj', 'Hitesh'], \n",
    "        'Age':[22, 32, 12, 52], \n",
    "        'Address':['Allahabad', 'Kannuaj', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['MCA', 'Phd', 'Bcom', 'B.hons'],\n",
    "        'Salary':[1000, 2000, 3000, 4000]} \n",
    " \n",
    "# Convert the dictionary into DataFrame  \n",
    "df = pd.DataFrame(data1,index=[0, 1, 2, 3])\n",
    " \n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data2, index=[2, 3, 6, 7]) \n",
    " \n",
    " \n",
    "print(df, \"\\n\\n\", df1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now we are going to apply ignore_index as an argument.\n",
    "\n",
    "# using ignore_index\n",
    "concat_df = pd.concat([df, df1], ignore_index=True)\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #########    ########   Concatenating DataFrame with group keys     ###########     #############\n",
    "\n",
    " =>  In order to concat dataframe with group keys, we override the column names with the use of the keys argument. \n",
    " \n",
    " =>  This indexes dataframes in concatenated dataframe.\n",
    "\n",
    "     >>>  pd.concat(frames, keys=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module\n",
    "import pandas as pd \n",
    "\n",
    "# Define a dictionary containing employee data \n",
    "data1 = {'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'], \n",
    "        'Age':[27, 24, 22, 32], \n",
    "        'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Msc', 'MA', 'MCA', 'Phd']} \n",
    "  \n",
    "# Define a dictionary containing employee data \n",
    "data2 = {'Name':['Abhi', 'Ayushi', 'Dhiraj', 'Hitesh'], \n",
    "        'Age':[17, 14, 12, 52], \n",
    "        'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Btech', 'B.A', 'Bcom', 'B.hons']} \n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df = pd.DataFrame(data1,index=[0, 1, 2, 3])\n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data2, index=[4, 5, 6, 7])\n",
    "\n",
    "print(df, \"\\n\\n\", df1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now we use keys as an argument.\n",
    "frames = [df, df1 ]\n",
    "\n",
    "res = pd.concat(frames, keys=['x', 'y'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ###############    ##############    Concatenating with mixed ndims    #############    ###############\n",
    "    \n",
    " =>  User can concatenate a mix of Series and DataFrame. \n",
    "\n",
    " =>  The Series will be transformed to DataFrame with the column name as the name of the Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module\n",
    "import pandas as pd \n",
    "\n",
    "# Define a dictionary containing employee data \n",
    "data1 = {'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'], \n",
    "        'Age':[27, 24, 22, 32], \n",
    "        'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Msc', 'MA', 'MCA', 'Phd']} \n",
    "  \n",
    "# Convert the dictionary into DataFrame  \n",
    "df = pd.DataFrame(data1,index=[0, 1, 2, 3])\n",
    "\n",
    "# creating a series\n",
    "s1 = pd.Series([1000, 2000, 3000, 4000], name='Salary')\n",
    "\n",
    "print(df, \"\\n\\n\", s1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to mix Series and dataframe together\n",
    "\n",
    "# combining series and dataframe\n",
    "concat_df = pd.concat([df, s1], axis=1)\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           ############     ##########   Merging dataframe  in pandas    ##########    ###########\n",
    "    \n",
    " =>  Pandas have options for high-performance in-memory merging and joining. \n",
    "\n",
    " =>  When we need to combine very large DataFrames, joins serve as a powerful way to perform these operations swiftly.\n",
    "        \n",
    " =>  Joins can only be done on two DataFrames at a time, denoted as left and right tables. \n",
    "\n",
    " =>  The key is the common column that the two DataFrames will be joined on. \n",
    "    \n",
    " =>  It’s a good practice to use keys which have unique values throughout the column to avoid unintended duplication of \n",
    "        row values. \n",
    "    \n",
    " =>  Pandas provide a single function, merge(), as the entry point for all standard database join operations between\n",
    "        DataFrame objects.\n",
    "\n",
    " =>  There are four basic ways to handle the join (inner, left, right, and outer), depending on which rows must retain \n",
    "        their data.\n",
    "    \n",
    " =>   pd.merge(left, right, how(str) = 'inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False,\n",
    "            \n",
    "           sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validator=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                  Parameters  for pd.merge()\n",
    "                                -------------------------------\n",
    "* left : DataFrame\n",
    "\n",
    "* right : DataFrame or named Series Object to merge with.\n",
    "\n",
    "* how : {'left', 'right', 'outer', 'inner'}, default 'inner'\n",
    "    \n",
    "       => Type of merge to be performed.\n",
    "\n",
    "    * left: use only keys from left frame, similar to a SQL left outer join; preserve key order.\n",
    "    \n",
    "    * right: use only keys from right frame, similar to a SQL right outer join; preserve key order.\n",
    "   \n",
    "    * outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\n",
    "    \n",
    "    * inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.\n",
    "\n",
    "* on : label or list\n",
    "    Column or index level names to join on. These must be found in both\n",
    "    DataFrames. If `on` is None and not merging on indexes then this defaults\n",
    "    to the intersection of the columns in both DataFrames.\n",
    "\n",
    "* left_on : label or list, or array-like\n",
    "    Column or index level names to join on in the left DataFrame. Can also\n",
    "    be an array or list of arrays of the length of the left DataFrame.\n",
    "    These arrays are treated as if they are columns.\n",
    "\n",
    "* right_on : label or list, or array-like\n",
    "    Column or index level names to join on in the right DataFrame. Can also\n",
    "    be an array or list of arrays of the length of the right DataFrame.\n",
    "    These arrays are treated as if they are columns.\n",
    "\n",
    "* left_index : bool, default False\n",
    "    Use the index from the left DataFrame as the join key(s). If it is a\n",
    "    MultiIndex, the number of keys in the other DataFrame (either the index\n",
    "    or a number of columns) must match the number of levels.\n",
    "\n",
    "* right_index : bool, default False\n",
    "    Use the index from the right DataFrame as the join key. Same caveats as\n",
    "    left_index.\n",
    "\n",
    "* sort : bool, default False\n",
    "    Sort the join keys lexicographically in the result DataFrame. If False,\n",
    "    the order of the join keys depends on the join type (how keyword).\n",
    "\n",
    "* suffixes : tuple of (str, str), default ('_x', '_y')\n",
    "    Suffix to apply to overlapping column names in the left and right\n",
    "    side, respectively. To raise an exception on overlapping columns use\n",
    "    (False, False).\n",
    "\n",
    "* copy : bool, default True\n",
    "    If False, avoid copy if possible.\n",
    "\n",
    "* indicator : bool or str, default False\n",
    "    If True, adds a column to output DataFrame called \"_merge\" with\n",
    "    information on the source of each row.\n",
    "    If string, column with information on source of each row will be added to\n",
    "    output DataFrame, and column will be named value of string.\n",
    "    Information column is Categorical-type and takes on a value of \"left_only\"\n",
    "    for observations whose merge key only appears in 'left' DataFrame,\n",
    "    \"right_only\" for observations whose merge key only appears in 'right'\n",
    "    DataFrame, and \"both\" if the observation's merge key is found in both.\n",
    "\n",
    "* validate :( str, optional) ->  If specified, checks if merge is of specified type.\n",
    "\n",
    "    * \"one_to_one\" or \"1:1\": check if merge keys are unique in both\n",
    "      left and right datasets.\n",
    "    * \"one_to_many\" or \"1:m\": check if merge keys are unique in left\n",
    "      dataset.\n",
    "    * \"many_to_one\" or \"m:1\": check if merge keys are unique in right\n",
    "      dataset.\n",
    "    * \"many_to_many\" or \"m:m\": allowed, but does not result in checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Code #1 : Merging a dataframe with one unique key combination\n",
    "\n",
    "# importing pandas module\n",
    "import pandas as pd \n",
    "\n",
    "# Define a dictionary containing employee data \n",
    "data1 = {'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "         'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'], \n",
    "        'Age':[27, 24, 22, 32],} \n",
    "  \n",
    "# Define a dictionary containing employee data \n",
    "data2 = {'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "         'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Btech', 'B.A', 'Bcom', 'B.hons']} \n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df = pd.DataFrame(data1)\n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data2) \n",
    " \n",
    "\n",
    "print(df, \"\\n\\n\", df1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are using .merge() with one unique key combination\n",
    "\n",
    "# using .merge() function\n",
    "result = pd.merge(df, df1, on='key')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Code #2: Merging dataframe using multiple join keys.\n",
    "\n",
    "# Define a dictionary containing employee data \n",
    "data1 = {'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "         'key1': ['K0', 'K1', 'K0', 'K1'],\n",
    "         'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'], \n",
    "        'Age':[27, 24, 22, 32],} \n",
    "  \n",
    "# Define a dictionary containing employee data \n",
    "data2 = {'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "         'key1': ['K0', 'K0', 'K0', 'K0'],\n",
    "         'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Btech', 'B.A', 'Bcom', 'B.hons']} \n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df = pd.DataFrame(data1)\n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data2) \n",
    " \n",
    "\n",
    "print(df, \"\\n\\n\", df1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we merge dataframe using multiple keys\n",
    "\n",
    "# merging dataframe using multiple keys\n",
    "res1 = pd.merge(df, df1, on=['key', 'key1'])\n",
    "\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #########    #########   Merging dataframe using how in an argument    ###########   ############\n",
    " \n",
    " =>  We use how argument to merge specifies how to determine which keys are to be included in the resulting table. \n",
    "    \n",
    " =>  If a key combination does not appear in either the left or right tables, the values in the joined table  \n",
    "       will be NA. \n",
    "      Here is a summary of the how options and their equivalent names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module\n",
    "import pandas as pd \n",
    "\n",
    "# Define a dictionary containing employee data \n",
    "data1 = {'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "         'key1': ['K0', 'K1', 'K0', 'K1'],\n",
    "         'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'], \n",
    "        'Age':[27, 24, 22, 32],} \n",
    "  \n",
    "# Define a dictionary containing employee data \n",
    "data2 = {'key': ['K0', 'K1', 'K2', 'K3'],\n",
    "         'key1': ['K0', 'K0', 'K0', 'K0'],\n",
    "         'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Btech', 'B.A', 'Bcom', 'B.hons']} \n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df = pd.DataFrame(data1)\n",
    "\n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data2) \n",
    " \n",
    "\n",
    "print(df, \"\\n\\n\", df1) \n",
    "\n",
    "#  Now we set how = 'left' in order to use keys from left frame only.\n",
    "\n",
    "\n",
    "\n",
    "# using keys from left frame\n",
    "res = pd.merge(df, df1, how='left', on=['key', 'key1'])\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now we set how = 'right' in order to use keys from right frame only.\n",
    "\n",
    "# using keys from right frame\n",
    "res1 = pd.merge(df, df1, how='right', on=['key', 'key1'])\n",
    "\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now we set how = 'outer' in order to get union of keys from dataframes.\n",
    "\n",
    "# getting union  of keys\n",
    "res2 = pd.merge(df, df1, how='outer', on=['key', 'key1'])\n",
    "\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we set how = 'inner' in order to get intersection of keys from dataframes.\n",
    "\n",
    "# getting intersection of keys\n",
    "res3 = pd.merge(df, df1, how='inner', on=['key', 'key1'])\n",
    "\n",
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              ##########    ##########   Joining DataFrame     ##########    #############     \n",
    "\n",
    " =>  In order to join dataframe, we use .join() function this function is used for combining the columns of two\n",
    "         potentially differently-indexed DataFrames into a single result DataFrame.\n",
    "    \n",
    " =>  df.join(df1, on=None, how='left', lsuffix='', rsuffix='', sort=false)\n",
    "\n",
    "   * how ('left', 'right', 'inner', 'outer')  ->  this says how to handle the joining operation of dataframes.\n",
    "   \n",
    "     => how='left'  ->  This will take all common rows and all individual rows of left (say=df) with NA values\n",
    "                             and join with df1.\n",
    "    \n",
    "     => how='right' ->  This will take all common rows and all individual rows of right (say=df1) with NA values and \n",
    "                             join with df.\n",
    "    \n",
    "     => how='outer' ->  This will take union of rows and joins all rows of both df and df1 and join them. \n",
    "    \n",
    "     => how='inner' ->  This will take an intersection of rows and joins only the common ones.\n",
    "\n",
    "\n",
    " =>  Dataframe 'df' with index as ('key'=['k0', 'k1', 'k2', 'k3']) and a second dataframe 'df1' contains 'key' as its column,\n",
    "        then we uses 'on' parameter to join both dataframes.\n",
    "    \n",
    " =>  Joining with on='key', rows of both datframes are joined with their corresponding key value.\n",
    "    \n",
    "    >>> df.join(df1, on='Key', how='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     ############  ############    Joining singly-indexed DataFrame with multi-indexed DataFrame  #############  ############\n",
    "    \n",
    " =>  In order to join singly indexed dataframe with multi-indexed dataframe, the level will match on the name\n",
    "         of the index of the singly-indexed frame against a level name of the multi-indexed frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module\n",
    "import pandas as pd \n",
    " \n",
    "# Define a dictionary containing employee data \n",
    "data1 = {'Name':['Jai', 'Princi', 'Gaurav'], \n",
    "        'Age':[27, 24, 22]} \n",
    "   \n",
    "# Define a dictionary containing employee data \n",
    "data2 = {'Address':['Allahabad', 'Kannuaj', 'Allahabad', 'Kanpur'], \n",
    "        'Qualification':['MCA', 'Phd', 'Bcom', 'B.hons']} \n",
    " \n",
    "# Convert the dictionary into DataFrame  \n",
    "df = pd.DataFrame(data1, index=pd.Index(['K0', 'K1', 'K2'], name='key'))\n",
    "\n",
    "index = pd.MultiIndex.from_tuples([('K0', 'Y0'), ('K1', 'Y1'),\n",
    "                                   ('K2', 'Y2'), ('K2', 'Y3')],\n",
    "                                   names=['key', 'Y'])\n",
    " \n",
    "# Convert the dictionary into DataFrame  \n",
    "df1 = pd.DataFrame(data2, index= index)\n",
    "\n",
    "\n",
    "print(df, \"\\n\\n\", df1)\n",
    "\n",
    "# Now we join singly indexed dataframe with multi-indexed dataframe\n",
    "\n",
    "# joining singly indexed with\n",
    "# multi indexed\n",
    "result = df.join(df1, how='inner')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      #########    #########   Pandas Series.str.cat() to concatenate string        ##########    ###########\n",
    "    \n",
    " =>  Pandas str.cat() is used to concatenate strings to the passed caller series of string. \n",
    " \n",
    " =>  Distinct values from a different series can be passed but the length of both the series has to be same. \n",
    "\n",
    " =>  .str has to be prefixed to differentiate it from the Python’s default method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module \n",
    "import pandas as pd \n",
    "\n",
    "# importing csv from link \n",
    "data = pd.read_csv(\"file.csv\") \n",
    "\n",
    "# making copy of team column \n",
    "new = data[\"Team\"].copy() \n",
    "\n",
    "# concatenating team with name column \n",
    "# overwriting name column \n",
    "data[\"Name\"]= data[\"Name\"].str.cat(new, sep =\", \") \n",
    "\n",
    "# display \n",
    "data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Example #2: Handling Null values\n",
    "\n",
    " =>  The most important part in analyzing data is handling null values. \n",
    "\n",
    " =>  str.cat() provides a way to handle null values through na_rep parameter. \n",
    "    \n",
    " =>  Whatever is passed to this parameter will be replaced at every occurrence of null value.\n",
    "        to replace null with this string.\n",
    "    \n",
    " =>  Output: As it can be seen in the data frame, at index position 4 and 5, there was NULL value which has been \n",
    "               replaced with “No College” and the string from Team column have been concatenated successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module \n",
    "import pandas as pd \n",
    "\n",
    "# importing csv from link \n",
    "data = pd.read_csv(\"file.csv\") \n",
    "\n",
    "# making copy of team column \n",
    "new = data[\"Team\"].copy() \n",
    "\n",
    "# string to replace null values with \n",
    "na_string =\"No College\"\n",
    "\n",
    "# concatenating team with name column \n",
    "# overwriting name column \n",
    "data[\"College\"]= data[\"College\"].str.cat(new, sep =\", \", na_rep = na_string) \n",
    "\n",
    "# display \n",
    "data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         ##########    #########  Pandas Index.append()    ##########    ############\n",
    "\n",
    " =>  Pandas Index.append() function is used to append a single or a collection of indices together.\n",
    "        \n",
    " =>  In case of collection of indices, all of them gets appended to the original index in the same order as they\n",
    "        are passed to the Index.append() function. The function returns an appended index.\n",
    "\n",
    "     >>>   Syntax: Index.append(other)\n",
    "            \n",
    "\n",
    "  =>       Parameters :\n",
    "\n",
    "               * other : Index or list/tuple of indices\n",
    "\n",
    "               * Returns : appended : bool or array_like (if axis is specified)  A single element array_like may be\n",
    "                                            converted to bool.\n",
    "                        \n",
    " => As we can see in the output, the second index i.e. df2 has been appended at the end of df1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Creating the first Index \n",
    "df1 = pd.Index([17, 69, 33, 5, 0, 74, 0]) \n",
    "\n",
    "# Creating the second Index \n",
    "df2 = pd.Index([11, 16, 54, 58]) \n",
    "\n",
    "# Print the first and second Index \n",
    "print(df1, \"\\n\", df2) \n",
    "\n",
    "# append df2 at the end of df1 \n",
    "df1.append(df2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Creating the first Index \n",
    "df1 = pd.Index(['Jan', 'Feb', 'Mar', 'Apr']) \n",
    "\n",
    "# Creating the second Index \n",
    "df2 = pd.Index(['May', 'Jun', 'Jul', 'Aug']) \n",
    "\n",
    "# Creating the third Index \n",
    "df3 = pd.Index(['Sep', 'Oct', 'Nov', 'Dec']) \n",
    "\n",
    "# Print the first, second and third Index \n",
    "print(df1, \"\\n\", df2, \"\\n\", df3) \n",
    "\n",
    "# We pass df2 and df3 as a list of \n",
    "# indexes to the append function \n",
    "df1.append([df2, df3]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ############     ###########    pandas Series.combine() method    ##########    #############\n",
    "    \n",
    " =>  Pandas Series.combine() is a series mathematical operation method. \n",
    "\n",
    " =>  This is used to combine two series into one. \n",
    "    \n",
    " =>  The shape of output series is same as the caller series. \n",
    "\n",
    " =>  The elements are decided by a function passed as parameter to combine() method. \n",
    "    \n",
    " =>  The shape of both series has to be same otherwise it will throw an error.\n",
    "\n",
    "    =>   Syntax: Series.combine(other, func, fill_value=nan)\n",
    "        \n",
    " =>  Parameters:\n",
    "\n",
    "        * other: other series or list type to be combined with caller series\n",
    "\n",
    "        * func: Function passed as parameter which will decide from which series the element should be put  \n",
    "                  at that index\n",
    "\n",
    "        * fill_value: integer value of level in case of multi index\n",
    "\n",
    "        * Return: Combined series with same shape as caller series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    *   Example #1:\n",
    "\n",
    " =>  In this example, two lists are made and converted into pandas series using .Series() method.\n",
    "      A function is made using lambda which checks which values is smaller in both series and returns whichever \n",
    "         is the smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module \n",
    "import pandas as pd \n",
    "\n",
    "# creating first series \n",
    "first =[1, 2, 5, 6, 3, 7, 11, 0, 4] \n",
    "\n",
    "# creating second series \n",
    "second =[5, 3, 2, 1, 3, 9, 21, 3, 1] \n",
    "\n",
    "# making series \n",
    "first = pd.Series(first) \n",
    "\n",
    "# making seriesa \n",
    "second = pd.Series(second) \n",
    "\n",
    "# calling .combine() method \n",
    "result = first.combine(second, (lambda x1, x2: x1 if x1 < x2 else x2)) \n",
    "\n",
    "# display \n",
    "result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    *   Example #2:\n",
    "\n",
    " =>  In this example, Null values are passed too using Numpy.nan method. Since series contains null values,\n",
    "         5 is passed to fill_value parameter to replace null values by 5. \n",
    "      \n",
    "    A lambda function is passed which will compare values in both series and will return the greater one.\n",
    "\n",
    " =>  Output: As shown in the output, the NaN values in the series were replaced by 5 before combining the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module \n",
    "import pandas as pd \n",
    "\n",
    "# importing numpy module \n",
    "import numpy as np \n",
    "\n",
    "# creating first series \n",
    "first =[1, 2, np.nan, 5, 6, 3, np.nan, 7, 11, 0, 4, 8] \n",
    "\n",
    "# creating second series \n",
    "second =[5, 3, 2, np.nan, 1, 3, 9, 21, 3, np.nan, 1, np.nan] \n",
    "\n",
    "# making series \n",
    "first = pd.Series(first) \n",
    "\n",
    "# making seriesa \n",
    "second = pd.Series(second) \n",
    "\n",
    "# calling .combine() method \n",
    "result = first.combine(second, func =(lambda x1, x2: x1 if x1 > x2 else x2), fill_value = 5) \n",
    "\n",
    "# display \n",
    "result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       ############    ##########   Adding a row at top of a Dataframe   ##########    #############\n",
    "# importing pandas module \n",
    "import pandas as pd \n",
    "\t\n",
    "# making data frame \n",
    "df = pd.read_csv(\"file.csv\") \n",
    "\n",
    "df.head(10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = pd.DataFrame({'Name':'Geeks', 'Team':'Boston', 'Number':3, \n",
    "\t\t\t\t\t\t'Position':'PG', 'Age':33, 'Height':'6-2', \n",
    "\t\t\t\t\t\t'Weight':189, 'College':'MIT', 'Salary':99999}, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tindex =[0]) \n",
    "\n",
    "df = pd.concat([new_row, df]).reset_index(drop = True) \n",
    "df.head(5) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ##########   ######   Pandas str.join() to join string/list elements with passed delimiter   ########    ########\n",
    "    \n",
    " =>  Pandas str.join() method is used to join all elements in list present in a series with passed delimiter.  \n",
    "       Since strings are also array of character (or List of characters), hence when this method is applied \n",
    "         on a series of strings, the string is joined at every character with the passed delimiter.\n",
    "\n",
    " =>  .str has to be prefixed every time before calling this method to differentiate it from the \n",
    "            Python’s default string method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  *  Example #1: Joining string elements\n",
    "\n",
    " =>  In this example, the str.join() method is used on Name column (Series of String). \n",
    "\n",
    " =>   As discussed earlier, a string is also an array of character and hence every character of string will be \n",
    "         joined with the passed separator using str.join() method.\n",
    "        \n",
    " =>  output:  As shown in the output image, the string in name column have been joined character wise with the \n",
    "                 passed separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module \n",
    "import pandas as pd \n",
    "\t\n",
    "# reading csv file from url \n",
    "data = pd.read_csv(\"file.csv\") \n",
    "\n",
    "# dropping null value columns to avoid errors \n",
    "data.dropna(inplace = True) \n",
    "\t\n",
    "# joining string and overwriting \n",
    "data[\"Name\"]= data[\"Name\"].str.join(\"-\") \n",
    "\n",
    "# display \n",
    "data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   *  Example #2: Joining elements of a list\n",
    "\n",
    " =>  In this example, the str.join() method is applied to a series of list. \n",
    "\n",
    " =>  The Data in team column is separated into list using str.split() method.\n",
    "    \n",
    "\n",
    " =>  output : As shown in the output images, the data was splitted into list using str.split() and then\n",
    "                \n",
    "                 the list was joined using str.join() with separator “_”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas module \n",
    "import pandas as pd \n",
    "\t\n",
    "# reading csv file from url \n",
    "data = pd.read_csv(\"file.csv\") \n",
    "\n",
    "# dropping null value columns to avoid errors \n",
    "data.dropna(inplace = True) \n",
    "\t\n",
    "# splitting string and overwriting \n",
    "data[\"Team\"]= data[\"Team\"].str.split(\"t\") \n",
    "\n",
    "# joining with \"_\" \n",
    "data[\"Team\"]= data[\"Team\"].str.join(\"_\") \n",
    "\n",
    "# display \n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ###########    ############  Join two text columns into a single column in Pandas   ############   ##########\n",
    "    \n",
    " *  Method #1: Using cat() function\n",
    "\n",
    " =>  We can also use different separators during join. e.g. -, _, ” ” etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Join two text columns into a single column in Pandas](https://www.geeksforgeeks.org/join-two-text-columns-into-a-single-column-in-pandas/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame({'Last': ['Gaitonde', 'Singh', 'Mathur'], \n",
    "\t\t\t\t'First': ['Ganesh', 'Sartaj', 'Anjali']}) \n",
    "\n",
    "print('Before Join') \n",
    "print(df, '\\n') \n",
    "\n",
    "print('After join') \n",
    "df['Name'] = df['First'].str.cat(df['Last'], sep =\" \") \n",
    "print(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  *  Method #2: Using lambda function\n",
    "\n",
    " =>  This method generalizes to an arbitrary number of string columns by replacing df[[‘First’, ‘Last’]] \n",
    "\n",
    "         with any column slice of your dataframe, e.g. df.iloc[:, 0:2].apply(lambda x: ‘ ‘.join(x), axis=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame({'Last': ['Gaitonde', 'Singh', 'Mathur'], \n",
    "\t\t\t\t'First': ['Ganesh', 'Sartaj', 'Anjali']}) \n",
    "\n",
    "print('Before Join') \n",
    "print(df, '\\n') \n",
    "\n",
    "print('After join') \n",
    "df['Name'] = df[['First', 'Last']].apply(lambda x: ' '.join(x), axis = 1) \n",
    "print(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  *  Method #3: Using + operator\n",
    "\n",
    " =>  We need to convert data frame elements into string before join. We can also use different separators during\n",
    "        join, e.g. -, _, ‘ ‘ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame({'Last': ['Gaitonde', 'Singh', 'Mathur'], \n",
    "\t\t\t\t'First': ['Ganesh', 'Sartaj', 'Anjali']}) \n",
    "\n",
    "print('Before Join') \n",
    "print(df, '\\n') \n",
    "\n",
    "print('After join') \n",
    "df['Name']= df[\"First\"].astype(str) +\" \"+ df[\"Last\"] \n",
    "print(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ##############   ##########   Working with Date and Time in Pandas    #############    ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  While working with data, encountering time series data is very usual. Pandas is a very useful tool while \n",
    "         working with time series data.\n",
    "\n",
    " =>  Pandas provide a different set of tools using which we can perform all the necessary tasks on date-time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code #1: Create a dates dataframe\n",
    "import pandas as pd \n",
    "\n",
    "# Create dates dataframe with frequency \n",
    "data = pd.date_range('1/1/2011', periods = 10, freq ='H') \n",
    "\n",
    "data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code #2: Create range of dates and show basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date and time with dataframe \n",
    "data = pd.date_range('1/1/2011', periods = 10, freq ='H') \n",
    "\n",
    "x = datetime.now() \n",
    "x.month, x.year \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  Datetime features can be divided into two categories.The first one time moments in a period \n",
    "       and second the time passed since a particular period. \n",
    "        \n",
    " => These features can be very useful to understand the patterns in the data.\n",
    "\n",
    " => Divide a given date into features –\n",
    "\n",
    "        * pandas.Series.dt.year returns the year of the date time.\n",
    "\n",
    "        * pandas.Series.dt.month returns the month of the date time.\n",
    "\n",
    "        * pandas.Series.dt.day returns the day of the date time.\n",
    "\n",
    "        * pandas.Series.dt.hour returns the hour of the date time.\n",
    "\n",
    "        * pandas.Series.dt.minute returns the minute of the date time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[All Datetime properties and features](https://pandas.pydata.org/pandas-docs/stable/reference/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code #3: Break data and time into separate features\n",
    "\n",
    "# Create date and time with dataframe \n",
    "rng = pd.DataFrame() \n",
    "rng['date'] = pd.date_range('1/1/2011', periods = 72, freq ='H') \n",
    "\n",
    "# Print the dates in dd-mm-yy format \n",
    "rng[:5] \n",
    "\n",
    "# Create features for year, month, day, hour, and minute \n",
    "rng['year'] = rng['date'].dt.year \n",
    "rng['month'] = rng['date'].dt.month \n",
    "rng['day'] = rng['date'].dt.day \n",
    "rng['hour'] = rng['date'].dt.hour \n",
    "rng['minute'] = rng['date'].dt.minute \n",
    "\n",
    "# Print the dates divided into features \n",
    "rng.head(3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date and time with dataframe \n",
    "rng = pd.DataFrame() \n",
    "rng['date'] = pd.date_range('1/1/2011', periods = 72, freq ='H') \n",
    "\n",
    "# Print the dates in dd-mm-yy format \n",
    "rng[:5] \n",
    "\n",
    "# Create features for year, month, day, hour, and minute \n",
    "rng['year'] = rng['date'].dt.year \n",
    "rng['month'] = rng['date'].dt.month \n",
    "rng['day'] = rng['date'].dt.day \n",
    "rng['hour'] = rng['date'].dt.hour \n",
    "rng['minute'] = rng['date'].dt.minute \n",
    "\n",
    "# Print the dates divided into features \n",
    "rng.head(3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  * Code 4: To get the present time, use Timestamp.now() and then convert timestamp to datetime and \n",
    "                directly access year, month or day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ################    ##############   Pandas Timestamp.timestamp   ##############    #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link](https://www.geeksforgeeks.org/python-pandas-timestamp-timestamp/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " => Pandas Timestamp.timestamp() function return the time expressed as the number of seconds that have passed \n",
    "         since January 1, 1970. That zero moment is known as the epoch.\n",
    "\n",
    "  => Syntax :Timestamp.timestamp()\n",
    "    \n",
    "      * Parameters : None\n",
    "\n",
    "      * Return : number of seconds since zero moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    * Example #1\n",
    "\n",
    " => Use Timestamp.timestamp() function to return the number of seconds that has passed since the zero moment\n",
    "       for the given Timestamp object.\n",
    "    \n",
    " =>  As we can see in the output, the Timestamp.timestamp() function has returned a float value indicating \n",
    "          this many seconds has passed since the epoch for the given Timestamp object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Create the Timestamp object \n",
    "ts = pd.Timestamp(year = 2011, month = 11, day = 21, \n",
    "\t\t\t\thour = 10, second = 49, tz = 'US/Central') \n",
    "\n",
    "# Print the Timestamp object \n",
    "print(ts) \n",
    "\n",
    "\n",
    "# Now we will use the Timestamp.timestamp() function to find the number of seconds that has passed.\n",
    "# return the number of seconds \n",
    "ts.timestamp() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   *  Example #2: \n",
    "    \n",
    "  =>  Use Timestamp.timestamp() function to return the number of seconds that has passed since the zero moment \n",
    "           for the given Timestamp object.\n",
    "    \n",
    "  =>  As we can see in the output, the Timestamp.timestamp() function has returned a float value indicating \n",
    "          this many seconds has passed since the epoch for the given Timestamp object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   *  Example #2: \n",
    "    \n",
    "  =>  Use Timestamp.timestamp() function to return the number of seconds that has passed since the zero moment \n",
    "           for the given Timestamp object.\n",
    "    \n",
    "  =>  As we can see in the output, the Timestamp.timestamp() function has returned a float value indicating \n",
    "          this many seconds has passed since the epoch for the given Timestamp object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   *  Example #2: \n",
    "    \n",
    "  =>  Use Timestamp.timestamp() function to return the number of seconds that has passed since the zero moment \n",
    "           for the given Timestamp object.\n",
    "    \n",
    "  =>  As we can see in the output, the Timestamp.timestamp() function has returned a float value indicating \n",
    "          this many seconds has passed since the epoch for the given Timestamp object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ##########    ##########   Pandas Timestamp.now   ###########    ###########\n",
    "    \n",
    " =>  Pandas Timestamp.now() function return the current time in the local timezone. \n",
    "\n",
    " =>  It is Equivalent to datetime.now([tz]).\n",
    "\n",
    "   =>   Syntax :Timestamp.now()\n",
    "    \n",
    "       *  Parameters : None\n",
    "\n",
    "       *  Return : Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  *  Example #1: \n",
    "    \n",
    " =>  Use Timestamp.now() function to return the current time in the local timezone.\n",
    "\n",
    " =>  As we can see in the output, the Timestamp.now() function has returned \n",
    "          the current time in the local timezone. It auto detects the local timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Create the Timestamp object \n",
    "ts = pd.Timestamp(year = 2011, month = 11, day = 21, \n",
    "\t\thour = 10, second = 49, tz = 'US/Central') \n",
    "\n",
    "# Print the Timestamp object \n",
    "print(ts) \n",
    "\n",
    "#  Now we will use the Timestamp.now() function to find the current time in the local timezone.\n",
    "\n",
    "# return the current time \n",
    "ts.now() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  * Example #2: \n",
    "    \n",
    " =>  Use Timestamp.now() function to return the current time in the local timezone.\n",
    "\n",
    " =>  As we can see in the output, the Timestamp.now() function has returned the \n",
    "            current time in the local timezone. \n",
    "        \n",
    " =>  It auto detects the local timezone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link](https://www.geeksforgeeks.org/python-pandas-timestamp-now/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Create the Timestamp object \n",
    "ts = pd.Timestamp(year = 2009, month = 5, day = 31, \n",
    "\thour = 4, second = 49, tz = 'Europe/Berlin') \n",
    "\n",
    "# Print the Timestamp object \n",
    "print(ts) \n",
    "\n",
    "# Now we will use the Timestamp.now() function to find the current time in the local timezone.\n",
    "# return the current time \n",
    "ts.now() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     ##########    ###########   Pandas Timestamp.isoformat     ##########    ############\n",
    "    \n",
    " =>  Pandas Timestamp.isoformat() function is used to convert the given Timestamp object into the ISO format.\n",
    "\n",
    " =>  Syntax :Timestamp.isoformat()\n",
    "        \n",
    "        *  Parameters : None\n",
    "\n",
    "        *  Return : date time as a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link](https://www.geeksforgeeks.org/python-pandas-timestamp-isoformat/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  *  Example #1: \n",
    "    \n",
    " =>  Use Timestamp.isoformat() function to convert the date in the given Timestamp object to ISO format.\n",
    "\n",
    " =>  As we can see in the output, the Timestamp.isoformat() function has returned the date in the ISO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Create the Timestamp object \n",
    "ts = pd.Timestamp(year = 2011, month = 11, day = 21, \n",
    "\t\t\t\thour = 10, second = 49, tz = 'US/Central') \n",
    "\n",
    "# Print the Timestamp object \n",
    "print(ts) \n",
    "\n",
    "# Now we will use the Timestamp.isoformat() function to convert the date in the given Timestamp object to ISO format.\n",
    "\n",
    "# convert to ISO format \n",
    "ts.isoformat() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           ##########    ###########   Pandas Timestamp.date   ##########    ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Pandas Timestamp.date](https://www.geeksforgeeks.org/python-pandas-timestamp-date/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  Pandas Timestamp.date() function return a datetime object with same year, month and day as that of\n",
    "         the given Timestamp object.\n",
    "    \n",
    " =>  Syntax : Timestamp.date()\n",
    "\n",
    "        * Parameters : None\n",
    "\n",
    "        * Return : date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  * Example #1:\n",
    "\n",
    " =>  Use Timestamp.date() function to return the date of the given Timestamp object as a datetime object.\n",
    "\n",
    " =>  As we can see in the output, the Timestamp.date() function has returned a datetime object \n",
    "         containing the same year, month and day as that of the given Timestamp object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Create the Timestamp object \n",
    "ts = pd.Timestamp(year = 2011, month = 11, day = 21, \n",
    "\t\thour = 10, second = 49, tz = 'US/Central') \n",
    "\n",
    "# Print the Timestamp object \n",
    "print(ts) \n",
    "\n",
    "# Now we will use the Timestamp.date() function to return the date as a datetime object.\n",
    "\n",
    "# return as a datetime object \n",
    "ts.date() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##########    ###########   Pandas Timestamp.replace  ##########    ############\n",
    "    \n",
    " =>  Pandas Timestamp.replace() function is used to replace the member values of the given Timestamp. \n",
    "\n",
    " =>  The function implements datetime.replace, and it also handles nanoseconds.\n",
    "    \n",
    " =>  Syntax :Timestamp.replace()\n",
    "\n",
    "      =>   Parameters :-\n",
    "\n",
    "              * year : int\n",
    "             \n",
    "              * month : int\n",
    "\n",
    "              * day : int\n",
    "\n",
    "              * hour : int\n",
    "\n",
    "              * minute : int\n",
    "\n",
    "              * second : int\n",
    "\n",
    "              * microsecond : int\n",
    "    \n",
    "              * nanosecond : int\n",
    "\n",
    "              *  tzinfo : int\n",
    "\n",
    "              * fold : int\n",
    "\n",
    "         *  Return : Timestamp with fields replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Pandas Timestamp.replace](https://www.geeksforgeeks.org/python-pandas-timestamp-replace/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " * Example #1:\n",
    "    \n",
    " => Use Timestamp.replace() function to replace the year value in the given Timestamp.\n",
    "\n",
    " =>  As we can see in the output, the Timestamp.replace() function has returned a Timestamp object \n",
    "         with year value equal to 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Create the Timestamp object \n",
    "ts = pd.Timestamp(year = 2011, month = 11, day = 21, \n",
    "\t\t\t\thour = 10, second = 49, tz = 'US/Central') \n",
    "\n",
    "# Print the Timestamp object \n",
    "print(ts) \n",
    "\n",
    "# Now we will use the Timestamp.replace() function to replace the current year in the object with 2019.\n",
    "\n",
    "# replace year \n",
    "ts.replace(year = 2019) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  * Example #2: \n",
    "    \n",
    " => Use Timestamp.replace() function to replace the year, month and hour value in the given Timestamp.\n",
    "\n",
    " =>  output : As we can see in the output, the Timestamp.replace() function has returned a Timestamp object with year \n",
    "                 value equal to 2019, month value equal to 12 and hour value equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Create the Timestamp object \n",
    "ts = pd.Timestamp(year = 2009, month = 5, day = 31, \n",
    "\t\t\t\thour = 4, second = 49, tz = 'Europe/Berlin') \n",
    "\n",
    "# Print the Timestamp object \n",
    "print(ts) \n",
    "\n",
    "#  Now we will use the Timestamp.replace() function to replace the current year, month and hour value in the object.\n",
    "\n",
    "# replace year, month and hour value \n",
    "ts.replace(year = 2019, month = 12, hour = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       ############    ##########    Pandas.to_datetime()   ############   ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  When a csv file is imported and a Data Frame is made, the Date time objects in the file are read as\n",
    "        a string object rather a Date Time object and Hence it’s very tough to perform operations like \n",
    "        Time difference on a string rather a Date Time object.\n",
    "        \n",
    " =>  Pandas to_datetime() method helps to convert string Date time into Python Date time object.\n",
    "\n",
    " =>  Return type: Date time object series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  *  Example #1: String to Date\n",
    "\n",
    " =>  In the following example, a csv file is read and the date column of Data frame is converted into Date Time \n",
    "        object from a string object.\n",
    "    \n",
    " =>  Output: As shown in the image, the Data Type of Date column was object but after using to_datetime(), \n",
    "               it got converted into a date time object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas package \n",
    "import pandas as pd \n",
    "\n",
    "# making data frame from csv file \n",
    "data = pd.read_csv(\"todatetime.csv\") \n",
    "\n",
    "# overwriting data after changing format \n",
    "data[\"Date\"]= pd.to_datetime(data[\"Date\"]) \n",
    "\n",
    "# info of data \n",
    "data.info() \n",
    "\n",
    "# display \n",
    "data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pandas.to_datetime() link](https://www.geeksforgeeks.org/python-pandas-to_datetime/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  *  Example #2: Exception while converting Time\n",
    "\n",
    " =>  Time object can also be converted with this method. But since in the Time column, a date isn’t specified\n",
    "       and hence Pandas will put Today’s date automatically in that case.\n",
    "    \n",
    " => Output: As shown in the output, a date (2018-07-07) that is Today’s date is already added with the Date time \n",
    "             object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "\n",
    "# making data frame from csv file \n",
    "data = pd.read_csv(\"todatetime.csv\") \n",
    "\n",
    "# overwriting data after changing format \n",
    "data[\"Time\"]= pd.to_datetime(data[\"Time\"]) \n",
    "\n",
    "# info of data \n",
    "data.info() \n",
    "\n",
    "# display \n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########   ############   pandas.date_range() method   ############    #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  pandas.date_range() is one of the general functions in Pandas which is used to return a fixed frequency \n",
    "           DatetimeIndex.\n",
    "    \n",
    " =>  Syntax: pandas.date_range(start=None, end=None, periods=None, freq=None, tz=None, normalize=False, name=None, \n",
    "                closed=None, **kwargs)\n",
    "\n",
    "\n",
    "    =>   Parameters :-\n",
    "\n",
    "          * start : Left bound for generating dates.\n",
    "\n",
    "          * end : Right bound for generating dates.\n",
    "\n",
    "          * periods : Number of periods to generate.\n",
    "\n",
    "          * freq : Frequency strings can have multiples, e.g. ‘5H’. See here for a list of frequency aliases.\n",
    "\n",
    "          *  tz : Time zone name for returning localized DatetimeIndex. \n",
    "                     By default, the resulting DatetimeIndex is timezone-naive.\n",
    "\n",
    "          * normalize : Normalize start/end dates to midnight before generating date range.\n",
    "\n",
    "          * name : Name of the resulting DatetimeIndex.\n",
    "\n",
    "          * closed : Make the interval closed with respect to the given frequency to the ‘left’, ‘right’, or both sides (None, the default).\n",
    "\n",
    "          * Returns: DatetimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ pandas.date_range() method ](https://www.geeksforgeeks.org/python-pandas-date_range-method/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "per1 = pd.date_range(start ='1-1-2018', \n",
    "\t\tend ='1-05-2018', freq ='5H') \n",
    "\n",
    "for val in per1: \n",
    "\tprint(val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "dRan1 = pd.date_range(start ='1-1-2018', \n",
    "\t\tend ='8-01-2018', freq ='M') \n",
    "\n",
    "dRan2 = pd.date_range(start ='1-1-2018', \n",
    "\t\tend ='11-01-2018', freq ='3M') \n",
    "\n",
    "print(dRan1, '\\n\\n', dRan2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Specify start and periods, the number of periods (days). \n",
    "dRan1 = pd.date_range(start ='1-1-2018', periods = 13) \n",
    "\n",
    "# Specify end and periods, the number of periods (days). \n",
    "dRan2 = pd.date_range(end ='1-1-2018', periods = 13) \n",
    "\n",
    "# Specify start, end, and periods; the frequency \n",
    "# is generated automatically (linearly spaced). \n",
    "dRan3 = pd.date_range(start ='01-03-2017', \n",
    "\t\t\tend ='1-1-2018', periods = 13) \n",
    "\n",
    "print(dRan1, \"\\n\\n\", dRan2, '\\n\\n', dRan3) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas as pd \n",
    "import pandas as pd \n",
    "\n",
    "# Specify start and periods, the number of periods (days). \n",
    "dRan1 = pd.date_range(start ='1-1-2018', \n",
    "\tperiods = 13, tz ='Asia/Tokyo') \n",
    "\n",
    "dRan1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ################    ##########    Plotting Graphs using pandas    ############    #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      ##############    ############   Some more topics of Pandas    #############    ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pandas.apply()](https://www.geeksforgeeks.org/python-pandas-apply/)\n",
    "\n",
    "\n",
    "[Apply function to every row in a Pandas DataFrame](https://www.geeksforgeeks.org/apply-function-to-every-row-in-a-pandas-dataframe/)\n",
    "\n",
    "[Pandas Series.apply()](https://www.geeksforgeeks.org/python-pandas-series-apply/)\n",
    "\n",
    "[Pandas dataframe.aggregate()](https://www.geeksforgeeks.org/python-pandas-dataframe-aggregate/)\n",
    "\n",
    "[Pandas dataframe.mean()](https://www.geeksforgeeks.org/python-pandas-dataframe-mean/)\n",
    "\n",
    "[Pandas Series.mean()](https://www.geeksforgeeks.org/python-pandas-series-mean/)\n",
    "\n",
    "[Pandas dataframe.mad()](https://www.geeksforgeeks.org/python-pandas-dataframe-mad/)\n",
    "\n",
    "\n",
    "[Pandas Series.mad() to calculate Mean Absolute Deviation of a Series](https://www.geeksforgeeks.org/python-pandas-series-mad-to-calculate-mean-absolute-deviation-of-a-series/)\n",
    "\n",
    "[Pandas dataframe.sem()](https://www.geeksforgeeks.org/python-pandas-dataframe-sem/)\n",
    "\n",
    "\n",
    "\n",
    "[Pandas Series.value_counts()](https://www.geeksforgeeks.org/python-pandas-series-value_counts/)\n",
    "\n",
    "\n",
    "[Pandas Index.value_counts()](https://www.geeksforgeeks.org/python-pandas-index-value_counts/)\n",
    "\n",
    "\n",
    "[For tutorial, click here](https://www.geeksforgeeks.org/pandas-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
